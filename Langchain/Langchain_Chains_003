{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMVVofhhiTDQGzXfDZM7tKn"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Chains allow us to link the output of one large language model call to the input of another call."],"metadata":{"id":"cnztX_3Hfd5G"}},{"cell_type":"code","source":["!pip install langchain openai chromadb tiktoken"],"metadata":{"id":"Ae49vr29feME","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692073835317,"user_tz":-540,"elapsed":47474,"user":{"displayName":"Gaurav Dahal","userId":"01909755880149023152"}},"outputId":"1c06a81b-489f-4183-cd1a-d7c396243f56"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting langchain\n","  Downloading langchain-0.0.264-py3-none-any.whl (1.5 MB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m1.1/1.5 MB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting openai\n","  Downloading openai-0.27.8-py3-none-any.whl (73 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting chromadb\n","  Downloading chromadb-0.4.5-py3-none-any.whl (402 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m402.8/402.8 kB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tiktoken\n","  Downloading tiktoken-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m78.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n","Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.19)\n","Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.8.5)\n","Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.2)\n","Collecting dataclasses-json<0.6.0,>=0.5.7 (from langchain)\n","  Downloading dataclasses_json-0.5.14-py3-none-any.whl (26 kB)\n","Collecting langsmith<0.1.0,>=0.0.11 (from langchain)\n","  Downloading langsmith-0.0.22-py3-none-any.whl (32 kB)\n","Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.5)\n","Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.23.5)\n","Collecting openapi-schema-pydantic<2.0,>=1.2 (from langchain)\n","  Downloading openapi_schema_pydantic-1.2.4-py3-none-any.whl (90 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pydantic<2,>=1 (from langchain)\n","  Downloading pydantic-1.10.12-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m98.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n","Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.0)\n","Collecting chroma-hnswlib==0.7.2 (from chromadb)\n","  Downloading chroma-hnswlib-0.7.2.tar.gz (31 kB)\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Collecting fastapi<0.100.0,>=0.95.2 (from chromadb)\n","  Downloading fastapi-0.99.1-py3-none-any.whl (58 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.4/58.4 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting uvicorn[standard]>=0.18.3 (from chromadb)\n","  Downloading uvicorn-0.23.2-py3-none-any.whl (59 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting posthog>=2.4.0 (from chromadb)\n","  Downloading posthog-3.0.1-py2.py3-none-any.whl (37 kB)\n","Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.7.1)\n","Collecting pulsar-client>=3.1.0 (from chromadb)\n","  Downloading pulsar_client-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m90.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting onnxruntime>=1.14.1 (from chromadb)\n","  Downloading onnxruntime-1.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m126.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tokenizers>=0.13.2 (from chromadb)\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m129.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pypika>=0.48.9 (from chromadb)\n","  Downloading PyPika-0.48.9.tar.gz (67 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Collecting overrides>=7.3.1 (from chromadb)\n","  Downloading overrides-7.4.0-py3-none-any.whl (17 kB)\n","Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.0.1)\n","Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.6.3)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.2.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n","Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n","  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n","  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n","Collecting starlette<0.28.0,>=0.27.0 (from fastapi<0.100.0,>=0.95.2->chromadb)\n","  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n","  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (23.5.26)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (23.1)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (3.20.3)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.12)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb) (1.16.0)\n","Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n","  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n","Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb)\n","  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n","Requirement already satisfied: python-dateutil>2.1 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb) (2.8.2)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from pulsar-client>=3.1.0->chromadb) (2023.7.22)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.4)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\n","Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (8.1.6)\n","Collecting h11>=0.8 (from uvicorn[standard]>=0.18.3->chromadb)\n","  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting httptools>=0.5.0 (from uvicorn[standard]>=0.18.3->chromadb)\n","  Downloading httptools-0.6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (428 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m428.8/428.8 kB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting python-dotenv>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n","  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n","Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb)\n","  Downloading uvloop-0.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.1/4.1 MB\u001b[0m \u001b[31m125.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n","  Downloading watchfiles-0.19.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m84.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb)\n","  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette<0.28.0,>=0.27.0->fastapi<0.100.0,>=0.95.2->chromadb) (3.7.1)\n","Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain)\n","  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n","Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n","  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi<0.100.0,>=0.95.2->chromadb) (1.3.0)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi<0.100.0,>=0.95.2->chromadb) (1.1.2)\n","Building wheels for collected packages: chroma-hnswlib, pypika\n","  Building wheel for chroma-hnswlib (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for chroma-hnswlib: filename=chroma_hnswlib-0.7.2-cp310-cp310-linux_x86_64.whl size=2285753 sha256=2692b3ccf6407ed62c45861f2c556285edad920a9a3525ecb63bfcb0bc342b67\n","  Stored in directory: /root/.cache/pip/wheels/11/2b/0d/ee457f6782f75315bb5828d5c2dc5639d471afbd44a830b9dc\n","  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53723 sha256=925eb3f181d9a429ff24d9ae56219e0b786c22a2506ac23d76674d92960e9904\n","  Stored in directory: /root/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\n","Successfully built chroma-hnswlib pypika\n","Installing collected packages: tokenizers, pypika, monotonic, websockets, uvloop, python-dotenv, pydantic, pulsar-client, overrides, mypy-extensions, marshmallow, humanfriendly, httptools, h11, chroma-hnswlib, backoff, watchfiles, uvicorn, typing-inspect, tiktoken, starlette, posthog, openapi-schema-pydantic, langsmith, coloredlogs, openai, onnxruntime, fastapi, dataclasses-json, langchain, chromadb\n","  Attempting uninstall: pydantic\n","    Found existing installation: pydantic 2.1.1\n","    Uninstalling pydantic-2.1.1:\n","      Successfully uninstalled pydantic-2.1.1\n","Successfully installed backoff-2.2.1 chroma-hnswlib-0.7.2 chromadb-0.4.5 coloredlogs-15.0.1 dataclasses-json-0.5.14 fastapi-0.99.1 h11-0.14.0 httptools-0.6.0 humanfriendly-10.0 langchain-0.0.264 langsmith-0.0.22 marshmallow-3.20.1 monotonic-1.6 mypy-extensions-1.0.0 onnxruntime-1.15.1 openai-0.27.8 openapi-schema-pydantic-1.2.4 overrides-7.4.0 posthog-3.0.1 pulsar-client-3.2.0 pydantic-1.10.12 pypika-0.48.9 python-dotenv-1.0.0 starlette-0.27.0 tiktoken-0.4.0 tokenizers-0.13.3 typing-inspect-0.9.0 uvicorn-0.23.2 uvloop-0.17.0 watchfiles-0.19.0 websockets-11.0.3\n"]}]},{"cell_type":"code","source":["from langchain.chat_models import ChatOpenAI\n","from langchain.prompts.chat import ChatPromptTemplate, HumanMessagePromptTemplate"],"metadata":{"id":"qpocoVQxu52_","executionInfo":{"status":"ok","timestamp":1692073841538,"user_tz":-540,"elapsed":2553,"user":{"displayName":"Gaurav Dahal","userId":"01909755880149023152"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["human_prompt = HumanMessagePromptTemplate.from_template(\"Make up a funny company name for a company that makes: {product}\")"],"metadata":{"id":"3K3k5A2Vu55D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["chat_prompt_template = ChatPromptTemplate.from_messages([human_prompt])"],"metadata":{"id":"_SEtLFjAu58z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["f = open(\"/content/api_key.txt\")\n","api_key = f.read()\n","chat = ChatOpenAI(openai_api_key = api_key)"],"metadata":{"id":"C4ABQxViu5_t","executionInfo":{"status":"ok","timestamp":1692073855327,"user_tz":-540,"elapsed":8,"user":{"displayName":"Gaurav Dahal","userId":"01909755880149023152"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["Now, what we're going to do is see how to connect this chat model along with the template to a chain object."],"metadata":{"id":"rvDvqnyfw5_s"}},{"cell_type":"code","source":["from langchain.chains import LLMChain"],"metadata":{"id":"sApVrSApu6Cx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["And so a large language model chain object takes in essentially two key parameters:\n","1. the large language model you want to connect it to, in this case, the chat model we just created\n","2. then any relevant prompts for that particular model call."],"metadata":{"id":"R9GqNv48xPO0"}},{"cell_type":"code","source":["chain = LLMChain(llm = chat, prompt = chat_prompt_template)\n","# we now have the chain ojbect"],"metadata":{"id":"DoVv_gsSfeNp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["result = chain.run(product = \"Computers\")"],"metadata":{"id":"JMvhUnebfeRV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["result"],"metadata":{"id":"_6Fqk0HZfeS1","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1691760549257,"user_tz":-540,"elapsed":436,"user":{"displayName":"Gaurav Dahal","userId":"01909755880149023152"}},"outputId":"7cf7ff48-94d6-4828-a331-b115a91e44a0"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Byte Me Computers'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["result = chain.run(product = \"AImodeled Performance Application\")"],"metadata":{"id":"qL94rSBayDgu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["result"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"cxN2eimCyDi1","executionInfo":{"status":"ok","timestamp":1691760581035,"user_tz":-540,"elapsed":22,"user":{"displayName":"Gaurav Dahal","userId":"01909755880149023152"}},"outputId":"8ce866fc-0ba7-454a-8246-c3d6906dd3ae"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'AI-rrific Perform-A-thon'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":16}]},{"cell_type":"markdown","source":["## SimpleSequentialChain\n","\n","Now that we understand how to use a singular chain object, we can now chain them together to create more complex functionality with langchain.\n","\n","----\n","\n","So what we're going to do is\n","- we're essentially going to grab multiple chain objects that themselves can take an input, some sort of argument call and then produce an output.\n","\n","- And we're going to wrap them inside a ***`SimpleSequentialChain`*** object.\n","(You pass them in as a python list.)"],"metadata":{"id":"lWZHsHnf7OaO"}},{"cell_type":"markdown","source":["From the overall user perspective, we're just providing an input to the simple sequential chain and then getting an output at the very end.\n","\n","Internally, what's happening is those inputs and outputs are being passed along from one LLMchain object to the other.\n","\n","And remember, each of these LLMchains is its own chat model call and its own system Prompt template human prompt template."],"metadata":{"id":"7_L-ee3p8fsJ"}},{"cell_type":"code","source":["from langchain.chains import LLMChain, SimpleSequentialChain"],"metadata":{"id":"ozwGKKxTyDml"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["llm = ChatOpenAI(openai_api_key=api_key)"],"metadata":{"id":"Y5yVcI7RyDpA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# TOPIC FOR A BLOG POST ---> [[ OUTLINE --> CREATE BLOG POST FROM OUTLINE ]] --> FINALOUTPUT (BLOG POST TEXT)\n","template = \"Give me a simple bulletpoint outline for a blog post on {topic}\"\n","first_prompt = ChatPromptTemplate.from_template(template)\n","chain_one = LLMChain(llm = llm, prompt = first_prompt)\n","#And so these three lines are the sort of thing you're going to be doing over and over again for creating LLM chains that you are passing in to a Sequential Chain."],"metadata":{"id":"zotGFLVmyDsI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Here, it's often common practice just to always refer to the same model call with the same APIs.\n","\n","But if you wanted to, technically speaking, each LLM chain object you make doesn't need to share the same model call.\n","\n","So if you really wanted to, you could have one block in the chain call OpenAI and then another block call something like vertex AI or Googlebot or something of that nature because maybe you like certain models for certain things or certain tasks."],"metadata":{"id":"Rv6tu0CK_TMt"}},{"cell_type":"code","source":["# Lets create another template\n","template2 = \"Write a blog post using this {outline}\"\n","second_prompt = ChatPromptTemplate.from_template(template2)\n","chain_two = LLMChain(llm=llm, prompt = second_prompt)"],"metadata":{"id":"Zg6oFHj2_FqW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["full_chain = SimpleSequentialChain(chains=[chain_one,chain_two],\n","                                   verbose= True)\n","\"\"\"\n","Note:\n","I would also recommend  that we say *verbose = True*.\n","So we can see the inputs and outputs of the LM chain objects.\n","If we don't say verbose is equal to true, it'll just seem to run for a little while and then give you the final output.\n","While you're learning and debugging things, its recommended to say \"verbose = True\".\n","\"\"\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":105},"id":"yi0A29w0_oOY","executionInfo":{"status":"ok","timestamp":1691764555502,"user_tz":-540,"elapsed":16,"user":{"displayName":"Gaurav Dahal","userId":"01909755880149023152"}},"outputId":"04d0da67-c1f1-4011-8ac9-bae9e05bbff4"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\nNote:\\nI would also recommend  that we say *verbose = True*.\\nSo we can see the inputs and outputs of the LM chain objects.\\nIf we don\\'t say verbose is equal to true, it\\'ll just seem to run for a little while and then give you the final output.\\nWhile you\\'re learning and debugging things, its recommended to say \"verbose = True\".\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":25}]},{"cell_type":"code","source":["result= full_chain.run(\"Cheesecake\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z9cH1epC_oba","executionInfo":{"status":"ok","timestamp":1691764792307,"user_tz":-540,"elapsed":40610,"user":{"displayName":"Gaurav Dahal","userId":"01909755880149023152"}},"outputId":"c7d64ded-6a87-47f4-e001-c2b39deb8636"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n","\u001b[36;1m\u001b[1;3m- Introduction: Briefly introduce the topic of cheesecake and its popularity as a delicious dessert.\n","- History of cheesecake: Explore the origins of cheesecake, dating back to ancient Greece and Rome.\n","- Types of cheesecake: Discuss the various types of cheesecake available, such as New York-style, no-bake, and fruit-topped variations.\n","- Ingredients and preparation: Highlight the key ingredients used in making a classic cheesecake, along with the basic steps involved in its preparation.\n","- Tips for a perfect cheesecake: Provide helpful tips and tricks for achieving the perfect texture, avoiding cracks, and ensuring a smooth and creamy consistency.\n","- Popular cheesecake flavors: List and describe some popular and unique cheesecake flavors, such as chocolate, strawberry, caramel, or pumpkin.\n","- Serving and garnishing ideas: Suggest different ways to serve and garnish cheesecake, such as with whipped cream, fresh fruit, or chocolate sauce.\n","- Healthier alternatives: Mention some healthier alternatives for those who want to enjoy cheesecake with reduced guilt, such as using Greek yogurt or using alternative sweeteners.\n","- Cheesecake around the world: Briefly touch upon how cheesecake is enjoyed in different countries and cultures, mentioning any unique variations.\n","- Conclusion: Wrap up the blog post by emphasizing the universal love for cheesecake and inviting readers to try making their own or visit a local bakery to indulge in this delicious treat.\u001b[0m\n","\u001b[33;1m\u001b[1;3mIntroduction: \n","Cheesecake, with its rich and creamy texture, is a dessert that has captured the hearts (and taste buds) of people all over the world. Whether it's the classic New York-style cheesecake or a unique fruity variation, this delectable treat never fails to satisfy our sweet cravings. In this blog post, we will explore the history, types, ingredients, and preparation of cheesecake, as well as provide tips, flavors, serving ideas, and healthier alternatives.\n","\n","History of Cheesecake:\n","The origins of cheesecake can be traced back to ancient Greece and Rome. The Greeks are believed to have served a version of cheesecake to athletes during the first Olympic Games in 776 BC. The Romans later adopted this delicacy and spread it throughout Europe. However, the cheesecake we enjoy today evolved during the 18th century in America, with the introduction of cream cheese by James Kraft. \n","\n","Types of Cheesecake:\n","Cheesecake comes in various forms, each with its own unique characteristics. The New York-style cheesecake is known for its dense and creamy texture, while the no-bake cheesecake offers a lighter and fluffier consistency. Fruit-topped variations, such as strawberry or blueberry cheesecake, add a burst of freshness and tanginess to the dessert. Other popular variations include chocolate cheesecake, caramel cheesecake, and even pumpkin cheesecake during the fall season.\n","\n","Ingredients and Preparation:\n","A classic cheesecake typically consists of cream cheese, sugar, eggs, and a graham cracker crust. The cream cheese provides the rich and smooth base, while sugar adds sweetness and eggs contribute to the structure. The crust, made from crushed graham crackers, adds a delicious crunch. To prepare a cheesecake, the ingredients are combined, poured into a crust-lined pan, and baked in a water bath to ensure even cooking and prevent cracking.\n","\n","Tips for a Perfect Cheesecake:\n","Creating the perfect cheesecake can be a bit tricky, but with a few tips and tricks, you can achieve a flawless result. Firstly, make sure all ingredients are at room temperature to ensure smooth blending. To avoid cracks on the surface, bake the cheesecake in a water bath and refrain from overmixing the batter. Additionally, cooling the cheesecake gradually after baking helps prevent cracking and ensures a creamy consistency.\n","\n","Popular Cheesecake Flavors:\n","While the classic plain cheesecake is always a hit, there are countless flavors to explore. Chocolate lovers can indulge in a rich and decadent chocolate cheesecake, while fruit enthusiasts can enjoy a refreshing strawberry or blueberry cheesecake. Caramel lovers can savor a sweet and gooey caramel cheesecake, and during the autumn season, pumpkin cheesecake offers a delightful blend of spices.\n","\n","Serving and Garnishing Ideas:\n","To elevate your cheesecake experience, consider serving and garnishing ideas that complement its flavors. A dollop of freshly whipped cream on top adds a light and airy touch, while a drizzle of chocolate sauce or caramel syrup enhances the sweetness. Fresh fruit, such as strawberries or raspberries, adds a burst of color and freshness.\n","\n","Healthier Alternatives:\n","For those who want to enjoy cheesecake with reduced guilt, there are healthier alternatives available. Using Greek yogurt instead of cream cheese can reduce the calorie and fat content while still maintaining a creamy texture. Alternatively, using alternative sweeteners like honey or maple syrup can help cut down on refined sugar.\n","\n","Cheesecake Around the World:\n","Cheesecake has become a beloved dessert in various countries and cultures, each with its own unique twist. In Italy, ricotta cheesecake is popular, while in Japan, a lighter and fluffier version called \"cotton cheesecake\" is enjoyed. In Germany, a traditional cheesecake called \"käsekuchen\" is made with quark cheese, and in Brazil, a creamy and tropical variation called \"romeu e julieta\" combines cheesecake with guava paste.\n","\n","Conclusion:\n","Cheesecake, with its rich history and countless variations, is a dessert that has won the hearts of people across the globe. Whether you decide to make your own cheesecake or visit a local bakery, this delightful treat is sure to satisfy your sweet tooth. So go ahead, indulge in a slice of creamy deliciousness and experience the universal love for cheesecake!\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n"]}]},{"cell_type":"code","source":["type(result)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G42QT5u-_oos","executionInfo":{"status":"ok","timestamp":1691764797429,"user_tz":-540,"elapsed":422,"user":{"displayName":"Gaurav Dahal","userId":"01909755880149023152"}},"outputId":"2c8190ec-521c-4997-99bb-191280e83f21"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["str"]},"metadata":{},"execution_count":30}]},{"cell_type":"code","source":["print(result)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":244},"id":"VQg4FrLcyDv4","executionInfo":{"status":"ok","timestamp":1691764983183,"user_tz":-540,"elapsed":14,"user":{"displayName":"Gaurav Dahal","userId":"01909755880149023152"}},"outputId":"a5716e4d-3a5a-4225-897f-d459e9179115"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Introduction: \\nCheesecake, with its rich and creamy texture, is a dessert that has captured the hearts (and taste buds) of people all over the world. Whether it\\'s the classic New York-style cheesecake or a unique fruity variation, this delectable treat never fails to satisfy our sweet cravings. In this blog post, we will explore the history, types, ingredients, and preparation of cheesecake, as well as provide tips, flavors, serving ideas, and healthier alternatives.\\n\\nHistory of Cheesecake:\\nThe origins of cheesecake can be traced back to ancient Greece and Rome. The Greeks are believed to have served a version of cheesecake to athletes during the first Olympic Games in 776 BC. The Romans later adopted this delicacy and spread it throughout Europe. However, the cheesecake we enjoy today evolved during the 18th century in America, with the introduction of cream cheese by James Kraft. \\n\\nTypes of Cheesecake:\\nCheesecake comes in various forms, each with its own unique characteristics. The New York-style cheesecake is known for its dense and creamy texture, while the no-bake cheesecake offers a lighter and fluffier consistency. Fruit-topped variations, such as strawberry or blueberry cheesecake, add a burst of freshness and tanginess to the dessert. Other popular variations include chocolate cheesecake, caramel cheesecake, and even pumpkin cheesecake during the fall season.\\n\\nIngredients and Preparation:\\nA classic cheesecake typically consists of cream cheese, sugar, eggs, and a graham cracker crust. The cream cheese provides the rich and smooth base, while sugar adds sweetness and eggs contribute to the structure. The crust, made from crushed graham crackers, adds a delicious crunch. To prepare a cheesecake, the ingredients are combined, poured into a crust-lined pan, and baked in a water bath to ensure even cooking and prevent cracking.\\n\\nTips for a Perfect Cheesecake:\\nCreating the perfect cheesecake can be a bit tricky, but with a few tips and tricks, you can achieve a flawless result. Firstly, make sure all ingredients are at room temperature to ensure smooth blending. To avoid cracks on the surface, bake the cheesecake in a water bath and refrain from overmixing the batter. Additionally, cooling the cheesecake gradually after baking helps prevent cracking and ensures a creamy consistency.\\n\\nPopular Cheesecake Flavors:\\nWhile the classic plain cheesecake is always a hit, there are countless flavors to explore. Chocolate lovers can indulge in a rich and decadent chocolate cheesecake, while fruit enthusiasts can enjoy a refreshing strawberry or blueberry cheesecake. Caramel lovers can savor a sweet and gooey caramel cheesecake, and during the autumn season, pumpkin cheesecake offers a delightful blend of spices.\\n\\nServing and Garnishing Ideas:\\nTo elevate your cheesecake experience, consider serving and garnishing ideas that complement its flavors. A dollop of freshly whipped cream on top adds a light and airy touch, while a drizzle of chocolate sauce or caramel syrup enhances the sweetness. Fresh fruit, such as strawberries or raspberries, adds a burst of color and freshness.\\n\\nHealthier Alternatives:\\nFor those who want to enjoy cheesecake with reduced guilt, there are healthier alternatives available. Using Greek yogurt instead of cream cheese can reduce the calorie and fat content while still maintaining a creamy texture. Alternatively, using alternative sweeteners like honey or maple syrup can help cut down on refined sugar.\\n\\nCheesecake Around the World:\\nCheesecake has become a beloved dessert in various countries and cultures, each with its own unique twist. In Italy, ricotta cheesecake is popular, while in Japan, a lighter and fluffier version called \"cotton cheesecake\" is enjoyed. In Germany, a traditional cheesecake called \"käsekuchen\" is made with quark cheese, and in Brazil, a creamy and tropical variation called \"romeu e julieta\" combines cheesecake with guava paste.\\n\\nConclusion:\\nCheesecake, with its rich history and countless variations, is a dessert that has won the hearts of people across the globe. Whether you decide to make your own cheesecake or visit a local bakery, this delightful treat is sure to satisfy your sweet tooth. So go ahead, indulge in a slice of creamy deliciousness and experience the universal love for cheesecake!'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":32}]},{"cell_type":"markdown","source":["# SequentialChains\n","\n","\n","SequentialChain is also quite easy to use, but gives us more flexibility on inputs and outputs and essentially returns a dictionary of all the internal inputs and outputs so you can access things.\n","\n","For example, maybe I do actually want to have the outline and save that somewhere so I know the reference outline for a future blog post and SequentialChains will allow us to do that.\n","\n","-----\n","\n","They allow us to have access to all the outputs from the internal LLMChain calls.\n","\n","That way we'll have a dictionary at the end in case we want to access outputs for other things like storing outputs from internal chains."],"metadata":{"id":"KyHmXscKDb-a"}},{"cell_type":"code","source":["from langchain.chains import SimpleSequentialChain, SequentialChain"],"metadata":{"id":"emRifUgPDa1u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Here --> Set up model"],"metadata":{"id":"DYlsWZ4NDbD8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Employee Performance Review (Input Text)\n","# review_text --> LLMCHAIN --> Summary\n","# take that Summary --> LLMCHAIN --> weaknesses\n","# Weaknesses --> LLMCHAIN --> Improvement plan"],"metadata":{"id":"JbgtEK3oDbIP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["template1 = \"Give a summary of this employees performance review\\n{review}\"\n","prompt1 = ChatPromptTemplate.from_template(template1)\n","chain1 = LLMChain(llm=llm, prompt = prompt1, output_key='review_summary')\n","# Here, something new since we're using SequentialChains is I'm going to actually specify the output_key.\n","# Essentially the name of the output and we'll call this review_summary.\n","# We'll have access to that in a final dictionary at the end."],"metadata":{"id":"M_BPv52uDbLp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["template2 = \"Identify key employee weaknesses in this review summary\\n{review_summary}\"\n","prompt2 = ChatPromptTemplate.from_template(template2)\n","chain2 = LLMChain(llm=llm, prompt = prompt2, output_key='weaknesses')"],"metadata":{"id":"MIdKu0N2DbPX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["template3 = \"Create a personlized plan to help address and fix these weaknesses\\n{weaknesses}\"\n","prompt3 = ChatPromptTemplate.from_template(template3)\n","chain3 = LLMChain(llm=llm, prompt = prompt3, output_key=\"final_plan\")"],"metadata":{"id":"he2V09BhyDzP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["employee_review = \"\"\"\n","Name: Harsh\n","Date: 27-july-23\n","\n","Learnings:\n","Today's focus as content creators was on honing our skills in optimizing content for specific target audiences based on valuable insights. Understanding our audience's preferences, interests, and behaviors allowed us to tailor our content to resonate more effectively with them, ultimately driving better engagement and results.\n","\n","Bottleneck:\n","Amidst the learning process, we faced a significant bottleneck—the sheer volume of content we were handling. As we strived to produce a high volume of content within tight deadlines, we noticed that the quality of our work was starting to suffer. This challenge hindered our ability to deliver content that truly met the expectations and needs of our target audience.\n","\n","Solution:\n","To overcome the bottleneck of declining content quality, we collectively brainstormed and devised a solution: adopting an alternative day release strategy. By spacing out our content releases, we could allocate more time and effort to ensure each piece of content receives the attention it deserves. This approach not only allowed us to maintain a consistent content schedule but also improved our content's overall quality and relevance to our audience.\n","\n","Celebration:\n","Amidst the hustle and challenges, today brought us a moment of celebration as a team. Despite the high volume of content and tight deadlines, we demonstrated exceptional teamwork and collaboration to ensure successful project delivery.\n","\"\"\""],"metadata":{"id":"E8SHII0WHd81"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Set Up SequentialChains\n","seq_chain = SequentialChain(chains = [chain1, chain2, chain3],\n","                            input_variables = [\"review\"],\n","                            output_variables=['review_summary','weaknesses',\"final_plan\"],\n","                            verbose= True)"],"metadata":{"id":"bcif6LIWHeUO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["results = seq_chain(employee_review)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iCHRn6W9HeaG","executionInfo":{"status":"ok","timestamp":1691769546461,"user_tz":-540,"elapsed":26016,"user":{"displayName":"Gaurav Dahal","userId":"01909755880149023152"}},"outputId":"8743b8c6-0b04-415d-ce30-8b1a8727a5c1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n"]}]},{"cell_type":"code","source":["type(results)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mhWjLtu9HehC","executionInfo":{"status":"ok","timestamp":1691769562903,"user_tz":-540,"elapsed":423,"user":{"displayName":"Gaurav Dahal","userId":"01909755880149023152"}},"outputId":"5f0fbe0d-21d7-420c-b546-a46341c85a69"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["dict"]},"metadata":{},"execution_count":44}]},{"cell_type":"code","source":["results.keys()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M3Zmsu2zHeoc","executionInfo":{"status":"ok","timestamp":1691769573484,"user_tz":-540,"elapsed":7,"user":{"displayName":"Gaurav Dahal","userId":"01909755880149023152"}},"outputId":"2e9b6099-af18-478d-f128-b1b26b3ba3a9"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["dict_keys(['review', 'review_summary', 'weaknesses', 'final_plan'])"]},"metadata":{},"execution_count":45}]},{"cell_type":"code","source":["results[\"review_summary\"]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":157},"id":"UE6vqQulHe4S","executionInfo":{"status":"ok","timestamp":1691769597961,"user_tz":-540,"elapsed":517,"user":{"displayName":"Gaurav Dahal","userId":"01909755880149023152"}},"outputId":"cf81f330-e0f3-42f9-963f-cd0020a704d5"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"Overall, Harsh's performance in this employee review was focused on learning and improving skills in optimizing content for specific target audiences. They faced a significant bottleneck due to the high volume of content they were handling, which affected the quality of their work. However, they were able to overcome this challenge by adopting an alternative day release strategy, allowing them to allocate more time and effort to each piece of content. Despite the challenges, Harsh demonstrated exceptional teamwork and collaboration, leading to successful project delivery.\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":46}]},{"cell_type":"code","source":["print(results[\"weaknesses\"])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5iQlwL2yHfBT","executionInfo":{"status":"ok","timestamp":1691769665511,"user_tz":-540,"elapsed":406,"user":{"displayName":"Gaurav Dahal","userId":"01909755880149023152"}},"outputId":"7e9ebc18-cbad-48c1-9f07-415192e6962e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Based on the review summary, the key employee weaknesses of Harsh can be identified as follows:\n","\n","1. Inability to handle high volume of content: Harsh faced a significant bottleneck due to the high volume of content they were handling. This suggests that they may struggle with managing their workload efficiently and effectively.\n","\n","2. Impact on the quality of work: The high volume of content affected the quality of Harsh's work. This weakness suggests that Harsh may need to improve their ability to maintain a high level of quality while working under pressure.\n","\n","Overall, the weaknesses identified in this review summary revolve around time management and maintaining quality under pressure.\n"]}]},{"cell_type":"code","source":["print(results[\"final_plan\"])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"onvvDyVHHfHS","executionInfo":{"status":"ok","timestamp":1691769693998,"user_tz":-540,"elapsed":527,"user":{"displayName":"Gaurav Dahal","userId":"01909755880149023152"}},"outputId":"a4d2cf70-c97f-49ec-d7d8-b647c3f645cb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["To address and fix these weaknesses, the following personalized plan can be implemented for Harsh:\n","\n","1. Time management training: Harsh should undergo training specifically focused on improving their time management skills. This training can include techniques such as prioritization, setting realistic deadlines, and delegating tasks when necessary. By learning how to manage their time effectively, Harsh will be better equipped to handle a high volume of content and avoid feeling overwhelmed.\n","\n","2. Stress management techniques: Harsh should also receive training on stress management techniques. This will help them remain calm and focused even when dealing with a heavy workload. Techniques such as deep breathing exercises, meditation, and time management strategies (mentioned above) can help Harsh stay organized and maintain a clear mind while working under pressure.\n","\n","3. Quality control measures: Harsh should be provided with clear guidelines and quality control measures to ensure that their work meets the required standards, even when facing a high volume of content. Regular feedback and performance evaluations can also help them identify areas for improvement and maintain a high level of quality in their work.\n","\n","4. Workflow optimization: Harsh should work with their supervisor or team to identify ways to optimize their workflow. This may involve streamlining processes, utilizing technology tools, or redistributing tasks among team members to improve efficiency. By identifying and implementing more efficient workflows, Harsh will be better able to handle a high volume of content without sacrificing quality.\n","\n","5. Time for self-care: It is important that Harsh takes regular breaks and prioritizes self-care to avoid burnout. Encourage them to schedule short breaks throughout the day and to engage in activities that help them relax and recharge. This will help them maintain their productivity and focus while working under pressure.\n","\n","6. Ongoing support and coaching: Harsh should receive ongoing support and coaching from their supervisor or a mentor. Regular check-ins and feedback sessions can provide them with guidance, support, and opportunities for growth. This will help them address their weaknesses and continue to improve their skills over time.\n","\n","By implementing this personalized plan, Harsh can address their weaknesses in time management and maintaining quality under pressure. With training, support, and the implementation of effective strategies, Harsh can become more efficient, productive, and capable of handling a high volume of content while maintaining a high level of quality in their work.\n"]}]},{"cell_type":"markdown","source":["# LLMRouterChain\n","\n","LM router chains can take in an input and then redirect it to the most appropriate LLMChain sequence.\n","\n","It does this by:-\n","1. attaching a description to each LLMChain separately. So, the router is going to accept multiple potential destination LLMChains\n","2. and then via a specialized prompt that it has internally, the router is going to read in the initial input\n","3. and then output a specific dictionary that matches up to one of the potential destination chains to continue processing."],"metadata":{"id":"iMoiPknzVilE"}},{"cell_type":"code","source":["# Students physics questions:\n","# \"How does a magnet work?\" --> simple question\n","# \"Explain Feynman diagram?\" --> advance question\n","# INPUT --> ROUTER --> LLM DECIDES CHAIN --> CHAIN ---> OUTPUT"],"metadata":{"id":"CfKOpvONVjAj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Here, maybe instead of choosing between the level of physics question, we can also actually choose topics like we have questions about math or questions about history or questions about physics or biology, and we want to route them to the appropriate LLMChain calls."],"metadata":{"id":"mDV44VlQK2Xz"}},{"cell_type":"code","source":["beginner_template = \"\"\"\n","You are a physics teacher who is really focused on\n","beginners and explaning complex concepts in simple to understand terms.\n","You assume no prior knowledge. Here is your question:\\n{input}\n","\"\"\""],"metadata":{"id":"l4WDIsJ9VjCx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["expert_template = \"\"\"\n","You are a physics professor who explains physics topics to advanced audience members.\n","YOu can assume anyone you answer has a PhD in Physics.\n","Here is your question:\\n{input}\n","\"\"\""],"metadata":{"id":"CZ8PsCrzVjGO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Here, what the router is going to do is it's going to decide if the question sounds like a beginner level question, I'll send it to the beginner chain call. If it sounds like a more advanced question, I'll explain it via the expert template call.\n","\n","\n","***NOTE:*** You can probably think of dozens of use cases for routing between different things."],"metadata":{"id":"NOaEqiwaMwtx"}},{"cell_type":"code","source":["# CREATING ROUTE PROMPT INFORMATION\n","# IT TAKES 3 THINGS. THEY ARE:\n","# [] --> NAME , DESCRIPTION, TEMPLATE (You would do this for as many templates as you have.)"],"metadata":{"id":"ZnpGuLwVVjIj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prompt_infos = [\n","    {\"name\": \"beginner physics\",\n","     \"description\": \"Answers basic physics questions\",\n","     \"template\": beginner_template},\n","      {\"name\": \"advance physics\",\n","     \"description\": \"Answer advance physics questions\",\n","     \"template\": expert_template},\n","\n","]"],"metadata":{"id":"ItbH0z8yVjLU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Begnning our converstational chain\n","from langchain.chat_models import ChatOpenAI\n","from langchain.prompts import ChatPromptTemplate\n","from langchain.chains import LLMChain"],"metadata":{"id":"1rw9rEurVjOc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# setting a new dictionary\n","destination_chains = {}\n","\n","for p_info in prompt_infos:\n","  name = p_info[\"name\"]\n","  prompt_template = p_info[\"template\"]\n","  prompt = ChatPromptTemplate.from_template(template= prompt_template)\n","  chain = LLMChain(llm = llm, prompt = prompt)\n","  destination_chains[name] = chain\n","\n","# So I have my destination chains, which is essentially all these LLMChain objects.\n","# Each of them being associated with some sort of template.\n","# Note: We haven't actually used the \"description\" yet, which is what we're going to do in a little bit when we set up the routing."],"metadata":{"id":"Q8JSsF75VjRe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# creating default prompt\n","default_prompt = ChatPromptTemplate.from_template(\"{input}\")\n","default_chain = LLMChain(llm = llm, prompt= default_prompt)"],"metadata":{"id":"5plhw0ncVjUU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# STEP 3:\n","# Setting up the Multi-prompt routing\n","from langchain.chains.router.multi_prompt_prompt import MULTI_PROMPT_ROUTER_TEMPLATE"],"metadata":{"id":"0z6nCCLQVjZe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(MULTI_PROMPT_ROUTER_TEMPLATE)"],"metadata":{"id":"RLffbc0VVjeD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1691802591201,"user_tz":-540,"elapsed":392,"user":{"displayName":"Gaurav Dahal","userId":"01909755880149023152"}},"outputId":"fbae80d2-5162-4f8f-d93b-7835b185a695"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Given a raw text input to a language model select the model prompt best suited for the input. You will be given the names of the available prompts and a description of what the prompt is best suited for. You may also revise the original input if you think that revising it will ultimately lead to a better response from the language model.\n","\n","<< FORMATTING >>\n","Return a markdown code snippet with a JSON object formatted to look like:\n","```json\n","{{{{\n","    \"destination\": string \\ name of the prompt to use or \"DEFAULT\"\n","    \"next_inputs\": string \\ a potentially modified version of the original input\n","}}}}\n","```\n","\n","REMEMBER: \"destination\" MUST be one of the candidate prompt names specified below OR it can be \"DEFAULT\" if the input is not well suited for any of the candidate prompts.\n","REMEMBER: \"next_inputs\" can just be the original input if you don't think any modifications are needed.\n","\n","<< CANDIDATE PROMPTS >>\n","{destinations}\n","\n","<< INPUT >>\n","{{input}}\n","\n","<< OUTPUT (must include ```json at the start of the response) >>\n","\n"]}]},{"cell_type":"markdown","source":["It basically tells the model a set of instructions, it explicitly tells it the kind of formatting it wants, like destination and next inputs.\n","\n","It reminds it for extra steering of the formatting and then it's going to have a candidate list of prompts and then the input that it's passing in.\n","\n","So we're going to build out an insert that list of destinations based off what we have up here for names and descriptions.\n","\n","So what we need to do is edit these destinations inside this giant multi prompt router template that LangChain is going to use internally.\n","\n","So let's insert those in as routing destinations and then we'll actually revisit the routing template prompt and it will probably make more sense once you see that."],"metadata":{"id":"0JbKqRypS969"}},{"cell_type":"code","source":["destinations = [f\"{p['name']}: {p['description']}\" for p in prompt_infos]"],"metadata":{"id":"Kzm5vFJ_Vjg0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["destinations"],"metadata":{"id":"1NlOu8XMVjjT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1691803145283,"user_tz":-540,"elapsed":462,"user":{"displayName":"Gaurav Dahal","userId":"01909755880149023152"}},"outputId":"aba3db40-43ae-47ac-9704-e625e1630ece"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['beginner physics: Answers basic physics questions',\n"," 'advance physics: Answer advance physics questions']"]},"metadata":{},"execution_count":26}]},{"cell_type":"markdown","source":["*`So what am I actually doing here?`*\n","\n","I'm just setting up some strings that are going to have the name and the description matching up just like it is in the prompt informations.\n","\n","\n","`*Question You might be thinking why not just use the original list and the dictionaries here?*`\n","\n","Well, remember, we're inserting this into some giant string input, so I need them in this format.\n","\n","So I can insert it into that routing template, which means to finally join this as a giant string."],"metadata":{"id":"2OcKb5JrVZAn"}},{"cell_type":"code","source":["destination_str = \"\\n\".join(destinations)"],"metadata":{"id":"vzZFzW26VbqS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(destination_str)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MIjZrEHVVbsl","executionInfo":{"status":"ok","timestamp":1691803472009,"user_tz":-540,"elapsed":9,"user":{"displayName":"Gaurav Dahal","userId":"01909755880149023152"}},"outputId":"296434bc-bb6d-4604-fc7a-72d1fa560ccc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["beginner physics: Answers basic physics questions\n","advance physics: Answer advance physics questions\n"]}]},{"cell_type":"markdown","source":["You can see it's those strings we built up, but now it's just one giant string with the new line as the separator.\n","\n","`*So why have it like this?*`\n","\n","Well, that's because that's exactly what we're going to input here in destinations. And those are the instructions it's following up here via the formatting.\n","\n","\n","\n","- So I had to build a dictionary to build out this list.\n","- Once I had that list, I could rejoin that list as this kind of giant text string\n","- that I can then input as my candidate prompts.\n","\n","So we have our routing destination set up."],"metadata":{"id":"HTC_LBV9WV0Z"}},{"cell_type":"code","source":["# setting up the prompt and do the chain call"],"metadata":{"id":"oafrXl-dVbvb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langchain.prompts import PromptTemplate\n","from langchain.chains.router.llm_router import LLMRouterChain, RouterOutputParser"],"metadata":{"id":"PuRGYAJ_Vbxm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Setting up router template\n","router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(destinations = destination_str)"],"metadata":{"id":"OIQtPU43Vb0j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(router_template)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UoUnW6mXVb4I","executionInfo":{"status":"ok","timestamp":1691803904988,"user_tz":-540,"elapsed":290,"user":{"displayName":"Gaurav Dahal","userId":"01909755880149023152"}},"outputId":"11ed2718-e0b2-43bf-87a6-948b344df928"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Given a raw text input to a language model select the model prompt best suited for the input. You will be given the names of the available prompts and a description of what the prompt is best suited for. You may also revise the original input if you think that revising it will ultimately lead to a better response from the language model.\n","\n","<< FORMATTING >>\n","Return a markdown code snippet with a JSON object formatted to look like:\n","```json\n","{{\n","    \"destination\": string \\ name of the prompt to use or \"DEFAULT\"\n","    \"next_inputs\": string \\ a potentially modified version of the original input\n","}}\n","```\n","\n","REMEMBER: \"destination\" MUST be one of the candidate prompt names specified below OR it can be \"DEFAULT\" if the input is not well suited for any of the candidate prompts.\n","REMEMBER: \"next_inputs\" can just be the original input if you don't think any modifications are needed.\n","\n","<< CANDIDATE PROMPTS >>\n","beginner physics: Answers basic physics questions\n","advance physics: Answer advance physics questions\n","\n","<< INPUT >>\n","{input}\n","\n","<< OUTPUT (must include ```json at the start of the response) >>\n","\n"]}]},{"cell_type":"markdown","source":["This was my original template.\n","\n","You could see it had a space holder for those destinations.\n","\n","Now, when I call (.format) on it with my own destination string i.e., (destination_str), I can see now I've inserted my own candidate prompts here based off the templates I defined earlier and again trying to get this all set up."],"metadata":{"id":"hDgQv7ggX5f9"}},{"cell_type":"code","source":["# creating a router prompt\n","router_prompt = PromptTemplate(template=router_template,\n","                               input_variables=[\"input\"],\n","                               output_parser= RouterOutputParser())"],"metadata":{"id":"meRaOdiOVb7v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Finally, our chain call\n","from langchain.chains.router import MultiPromptChain"],"metadata":{"id":"JXhntVnhVb-T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["router_chain = LLMRouterChain.from_llm(llm, router_prompt)"],"metadata":{"id":"QCb2kmWYVcI1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["chain = MultiPromptChain(router_chain=router_chain,\n","                         destination_chains=destination_chains,\n","                         default_chain=default_chain,\n","                         verbose=True)"],"metadata":{"id":"ljLIhiLpVcLR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["chain.run(\"How do magnets work?\")"],"metadata":{"id":"ZboV3gmeyD1h","colab":{"base_uri":"https://localhost:8080/","height":385},"executionInfo":{"status":"ok","timestamp":1691804448216,"user_tz":-540,"elapsed":13773,"user":{"displayName":"Gaurav Dahal","userId":"01909755880149023152"}},"outputId":"b1bfc126-e11d-4b79-e92e-c4b6af91b370"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:279: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["beginner physics: {'input': 'How do magnets work?'}\n","\u001b[1m> Finished chain.\u001b[0m\n"]},{"output_type":"execute_result","data":{"text/plain":["\"Great question! Let's dive into the fascinating world of magnets. Magnets are objects that have the ability to attract or repel certain materials, like iron or steel. The way magnets work is based on a force called magnetism.\\n\\nAt the heart of a magnet's power lies tiny particles called atoms. These atoms have even smaller particles called electrons, which are constantly moving around them. Now, here's where the magic happens!\\n\\nIn certain materials, like iron, the electrons are arranged in a way that creates tiny magnetic fields. These magnetic fields are like invisible forces that surround each atom. Normally, these tiny magnetic fields cancel each other out, so the material as a whole doesn't exhibit any noticeable magnetism.\\n\\nHowever, in a magnet, the atoms are arranged in a special way. This arrangement aligns the magnetic fields of the atoms, making them all point in the same direction. This alignment creates a stronger magnetic field that extends beyond the surface of the magnet.\\n\\nWhen you bring two magnets close together, something interesting happens. Remember that magnets have two ends, called poles: a north pole and a south pole. Similar to how opposite charges attract and like charges repel, opposite poles of magnets attract each other, while like poles repel each other.\\n\\nThe reason for this attraction and repulsion lies in the magnetic fields produced by the aligned atoms. When opposite poles are brought together, their magnetic fields combine and strengthen, pulling the magnets towards each other. On the other hand, when like poles come close, their magnetic fields oppose each other, creating a force that pushes the magnets apart.\\n\\nIt's like two teams pulling on a rope: if the teams are pulling in opposite directions, the rope gets stretched, but if they are pulling in the same direction, the rope breaks or pushes them away.\\n\\nThis behavior of magnets is what allows them to stick to metal objects or attract each other. You might have also noticed that magnets can attract certain materials but not others. This is because the alignment of the atoms in those materials doesn't allow them to generate magnetic fields that interact strongly with the magnet's field.\\n\\nSo, in a nutshell, magnets work due to the alignment of atoms and the resulting magnetic fields they produce. This alignment creates attractive and repulsive forces between magnets and certain materials. It's a captivating force of nature that scientists are still exploring and learning more about!\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":38}]},{"cell_type":"code","source":["chain.run(\"Please explain Feynman Diagrams\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":330},"id":"I3sbw9lQZ-bT","executionInfo":{"status":"ok","timestamp":1691804522394,"user_tz":-540,"elapsed":12199,"user":{"displayName":"Gaurav Dahal","userId":"01909755880149023152"}},"outputId":"7f111146-4212-47be-ee77-cd7310a58922"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n","advance physics: {'input': 'Please explain Feynman Diagrams'}\n","\u001b[1m> Finished chain.\u001b[0m\n"]},{"output_type":"execute_result","data":{"text/plain":["'Feynman diagrams are graphical representations used in quantum field theory to depict the interactions between elementary particles. They were developed by physicist Richard Feynman in the 1940s as a way to better understand and calculate the probabilities of particle interactions.\\n\\nAt their core, Feynman diagrams provide a pictorial language to describe the behavior of particles and their interactions. They consist of lines and vertices, where each line represents a particle and each vertex represents an interaction between particles.\\n\\nThe lines in a Feynman diagram can be of different types, depending on the nature of the particle being represented. For example, solid lines typically represent fermions, such as electrons or quarks, while wavy or curly lines represent bosons, such as photons or gluons.\\n\\nThe vertices in a Feynman diagram represent interactions between particles. These interactions can involve the exchange of other particles, which are represented by lines connecting the vertices. For instance, a photon can mediate the electromagnetic interaction between two charged particles, and this is depicted by a line representing the photon connecting the respective vertices.\\n\\nThe structure and arrangement of the lines and vertices in a Feynman diagram provide valuable information about the likelihood of a particular particle interaction occurring. By assigning mathematical expressions called propagators and coupling constants to the lines and vertices, respectively, physicists can calculate the probability amplitude for a given process to happen.\\n\\nFeynman diagrams also allow for the visualization of conservation laws, such as conservation of energy and momentum. The lines in a diagram can be traced to determine the initial and final states of the particles involved, ensuring that the laws of physics are obeyed throughout the interaction.\\n\\nWhile Feynman diagrams are not a literal representation of the physical processes occurring, they serve as powerful tools for making calculations and predictions in quantum field theory. By utilizing Feynman diagrams, physicists can understand and analyze particle interactions, helping to unlock the mysteries of the subatomic world.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":39}]},{"cell_type":"markdown","source":["All you need to really do is come up here, set up your templates, and then set up your prompt info list with these dictionaries for the name and description.\n","\n","The key thing to keep in mind is this name and description are pretty crucial in helping the model decide what template it should be referring to.\n","\n","And we now understand that because we can see that all Langchain is doing is it's going to insert those candidate prompts here and have the large language model initially decide.\n","\n","So use that knowledge of the name and the description and the fact that it's going into that initial prompt as leveraging, helping the model decide what template to choose.\n","\n","----"],"metadata":{"id":"q2Ae9H8ZaVg_"}},{"cell_type":"markdown","source":["# TransformChain\n","\n","Transform chain is going to allow us to insert our own functionality using Python functions to do any sort of transformations we want inside of a chain call."],"metadata":{"id":"tP1_d7j0YFvX"}},{"cell_type":"code","source":["yelp_review = open(\"/content/yelp_review.txt\").read()"],"metadata":{"id":"UZ-B8KTFYGK7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# print(yelp_review.split(\"REVIEW:\")[-1].lower())"],"metadata":{"id":"DHvmNU0VYGNv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Take the INPUT --> CUSTOM PYTHON TRANSFORMATION --> LLMChain"],"metadata":{"id":"Q0-XkWEDYGQt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langchain.chains import LLMChain, TransformChain, SimpleSequentialChain"],"metadata":{"id":"Xw_nQTWXYGTP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langchain.chat_models import ChatOpenAI"],"metadata":{"id":"PAfxta88YGV6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langchain.prompts import ChatPromptTemplate"],"metadata":{"id":"2K8hduKxYGYd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["llm = ChatOpenAI(openai_api_key= api_key)"],"metadata":{"id":"-NS-ZcdFb2dM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#  First we functionalize\n","# its going to take inputs that are dictionary and give dict output\n","def transformer_fun(inputs:dict) -> dict:\n","  text = inputs[\"text\"]\n","  only_review_text = text.split(\"REVIEW:\")[-1]\n","  lower_case_text = only_review_text.lower()\n","  return {\"output\": lower_case_text}\n"],"metadata":{"id":"H4gjkA5DYGbo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["transform_chain = TransformChain(input_variables=[\"text\"],\n","                                 output_variables=[\"output\"],\n","                                 transform= transformer_fun)"],"metadata":{"id":"WDfZlNcGYGeE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["template = \"Create a one sentence summary of this review:\\n{review}\""],"metadata":{"id":"mh1kgXoDbg0G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prompt = ChatPromptTemplate.from_template(template)"],"metadata":{"id":"tPjxF5rqbg38"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["summary_chain = LLMChain(llm =llm, prompt = prompt, output_key= \"review_summary\")"],"metadata":{"id":"4XxsFIh9bg6_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sequential_chain = SimpleSequentialChain(chains=[transform_chain, summary_chain], verbose= True)"],"metadata":{"id":"dmJOXY-Zbg9p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["result = sequential_chain(yelp_review)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yRB8onNKbhAx","executionInfo":{"status":"ok","timestamp":1691973022501,"user_tz":-540,"elapsed":1554,"user":{"displayName":"Gaurav Dahal","userId":"01909755880149023152"}},"outputId":"3b02439c-7826-4dad-f3ee-45b8a71871fc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n","\u001b[36;1m\u001b[1;3m\n","oh my goodness, where do i begin? this restaurant is absolutely phenomenal! i went there last night with my friends, and we were blown away by the experience!\n","\n","first of all, the ambiance is out of this world! the moment you step inside, you're greeted with a warm and inviting atmosphere. the decor is stunning, and it immediately sets the tone for an unforgettable dining experience.\n","\n","now, let's talk about the food! wow, just wow! the menu is a paradise for food lovers. every dish we ordered was a masterpiece. the flavors were bold, vibrant, and exploded in our mouths. from starters to desserts, every bite was pure bliss!\n","\n","their seafood platter is a must-try! the freshness of the seafood is unmatched, and the presentation is simply stunning. i have never tasted such delicious and perfectly cooked seafood in my life. it's a seafood lover's dream come true!\n","\n","the service was exemplary. the staff was attentive, friendly, and extremely knowledgeable about the menu. they went above and beyond to ensure that we had the best dining experience possible.\n","\n","and let's not forget about the desserts! oh my, oh my! i had their signature chocolate lava cake, and it was pure heaven. the cake was moist, and when i cut into it, the warm chocolate ooozed out, creating an explosion of flavor in my mouth. it was like a symphony of chocolatey goodness!\n","\n","in conclusion, this restaurant is a hidden gem! if you want to indulge in a memorable dining experience, do yourself a favor and visit this place. you won't regret it! i can't wait to go back and try more of their delectable dishes. kudos to the entire team for creating such a culinary haven!\n","\n","all i can say is... woohoo!\u001b[0m\n","\u001b[33;1m\u001b[1;3mThis review raves about the phenomenal restaurant experience, from the inviting ambiance and stunning decor to the bold and vibrant flavors of the food, particularly the must-try seafood platter, topped off with exemplary service and heavenly desserts.\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n"]}]},{"cell_type":"code","source":["result[\"output\"]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":87},"id":"JwrnJuCHbhDV","executionInfo":{"status":"ok","timestamp":1691973063815,"user_tz":-540,"elapsed":15,"user":{"displayName":"Gaurav Dahal","userId":"01909755880149023152"}},"outputId":"7b7cc7c3-6095-49c0-b57f-44e217990f0c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'This review raves about the phenomenal restaurant experience, from the inviting ambiance and stunning decor to the bold and vibrant flavors of the food, particularly the must-try seafood platter, topped off with exemplary service and heavenly desserts.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":30}]},{"cell_type":"markdown","source":["# OpenAI Function Calling with Langchain"],"metadata":{"id":"4DXfr4jYvJ2L"}},{"cell_type":"code","source":["from langchain.chat_models import ChatOpenAI"],"metadata":{"id":"TXqdioVHbhGU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["llm = ChatOpenAI(model = \"gpt-3.5-turbo\", openai_api_key=api_key)"],"metadata":{"id":"J9x1uPg1voHw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Scientist():\n","\n","  def __init__(self, first_name, last_name):\n","    self.first_name = first_name\n","    self.last_name = last_name"],"metadata":{"id":"6RHcJmIrvoLp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["With this new function capability inside of OpenAI, I can take advantage of the fact that it can actually understand a Json schema and be a lot more steerable.\n","\n","In order to do this, I do need to define a Json schema that matches up to what I need."],"metadata":{"id":"neB0MHLkwYMI"}},{"cell_type":"code","source":["json_schema = {'title':'Scientist',\n","               'description': 'Information about a famous scientist',\n","               'type':'object',\n","               'properties': {\n","                  'first_name':{'title':'First_Name',\n","                                'description':'First name of scientist',\n","                                'type':'string',\n","                  'last_name':{'title':'Last_Name',\n","                               'description':'Last name of scientist',\n","                               'type':'string'}\n","\n","                  },\n","                  'required':['first_name', 'last_name']\n","               }}"],"metadata":{"id":"M7JPAEmuvoOq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["So I have this Json schema. We have an overall title and an overall description.\n","\n","So when I ask a question of the large language model, it understands that I need to start formatting it in this sort of sense.\n","\n","And then this is associated with an object above that I made up.\n","\n","And then I have properties and this is what the large language model really uses to align things. So typically, if I asked it for the first and last name of a famous scientist, it would say something like, \"A famous scientist is Albert Einstein.\"\n","\n","But what I need to do is just return a dictionary that says first_name Albert, last_name Einstein, and then give me that in that format and then later on I can parse that format and put it into this class call.\n","\n","So I have my Json schema. I can also specify what is required to be returned.\n","\n","You don't need to require all the properties to be returned, although in general I have found that it works better when you require everything."],"metadata":{"id":"saHkmZu8yaF5"}},{"cell_type":"code","source":["template = \"Name a famous {country} scientist\""],"metadata":{"id":"3zf-g6SHvoRH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langchain.chains.openai_functions import create_structured_output_chain"],"metadata":{"id":"rgeSajZyzF5W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["chat_prompt =  ChatPromptTemplate.from_template(template)"],"metadata":{"id":"GtyOozQkzF8P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["chain = create_structured_output_chain(json_schema, llm, chat_prompt, verbose = True)"],"metadata":{"id":"DpXp_wLkzF_F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["result = chain.run(country = \"American\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":460},"id":"Ry1kUl-_zGER","executionInfo":{"status":"error","timestamp":1691979459154,"user_tz":-540,"elapsed":991,"user":{"displayName":"Gaurav Dahal","userId":"01909755880149023152"}},"outputId":"4a992237-cabf-419a-f31a-6d9d2a4d4f01"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","\u001b[1m> Entering new LLMChain chain...\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mHuman: Name a famous American scientist\u001b[0m\n"]},{"output_type":"error","ename":"InvalidRequestError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mInvalidRequestError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-53-9dcc34d43e20>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcountry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"American\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 480\u001b[0;31m             return self(kwargs, callbacks=callbacks, tags=tags, metadata=metadata)[\n\u001b[0m\u001b[1;32m    481\u001b[0m                 \u001b[0m_output_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m             ]\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, include_run_info)\u001b[0m\n\u001b[1;32m    280\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m         \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m         final_outputs: Dict[str, Any] = self.prep_outputs(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, include_run_info)\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m             outputs = (\n\u001b[0;32m--> 276\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mnew_arg_supported\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m                 \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mrun_manager\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCallbackManagerForChainRun\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     ) -> Dict[str, str]:\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, input_list, run_manager)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;34m\"\"\"Generate LLM result from inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprep_prompts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         return self.llm.generate_prompt(\n\u001b[0m\u001b[1;32m    103\u001b[0m             \u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chat_models/base.py\u001b[0m in \u001b[0;36mgenerate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    413\u001b[0m     ) -> LLMResult:\n\u001b[1;32m    414\u001b[0m         \u001b[0mprompt_messages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_messages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_messages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m     async def agenerate_prompt(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chat_models/base.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, **kwargs)\u001b[0m\n\u001b[1;32m    308\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mrun_managers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mrun_managers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_llm_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m         flattened_outputs = [\n\u001b[1;32m    312\u001b[0m             \u001b[0mLLMResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerations\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mllm_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllm_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chat_models/base.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, **kwargs)\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m                 results.append(\n\u001b[0;32m--> 300\u001b[0;31m                     self._generate_with_cache(\n\u001b[0m\u001b[1;32m    301\u001b[0m                         \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                         \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chat_models/base.py\u001b[0m in \u001b[0;36m_generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    445\u001b[0m                 )\n\u001b[1;32m    446\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnew_arg_supported\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 447\u001b[0;31m                 return self._generate(\n\u001b[0m\u001b[1;32m    448\u001b[0m                     \u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m                 )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chat_models/openai.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, messages, stop, run_manager, stream, **kwargs)\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0mmessage_dicts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_message_dicts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m         response = self.completion_with_retry(\n\u001b[0m\u001b[1;32m    341\u001b[0m             \u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessage_dicts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chat_models/openai.py\u001b[0m in \u001b[0;36mcompletion_with_retry\u001b[0;34m(self, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_completion_with_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_combine_llm_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mllm_outputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36mwrapped_f\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mwrapped_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mretry_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mWrappedFn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0mretry_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRetryCallState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_object\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m             \u001b[0mdo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDoAttempt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36miter\u001b[0;34m(self, retry_state)\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0mis_explicit_retry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfut\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfailed\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfut\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTryAgain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_explicit_retry\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfut\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    449\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m                 \u001b[0;31m# Break a reference cycle with the exception in self._exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDoAttempt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# noqa: B902\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m                     \u001b[0mretry_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chat_models/openai.py\u001b[0m in \u001b[0;36m_completion_with_retry\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mretry_decorator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_completion_with_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_completion_with_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_resources/chat_completion.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTryAgain\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_resources/abstract/engine_api_resource.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    151\u001b[0m         )\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         response, _, api_key = requestor.request(\n\u001b[0m\u001b[1;32m    154\u001b[0m             \u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0mrequest_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         )\n\u001b[0;32m--> 298\u001b[0;31m         \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_interpret_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    698\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m             return (\n\u001b[0;32m--> 700\u001b[0;31m                 self._interpret_response_line(\n\u001b[0m\u001b[1;32m    701\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    761\u001b[0m         \u001b[0mstream_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstream\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"error\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstream_error\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mrcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 763\u001b[0;31m             raise self.handle_error_response(\n\u001b[0m\u001b[1;32m    764\u001b[0m                 \u001b[0mrbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m             )\n","\u001b[0;31mInvalidRequestError\u001b[0m: Invalid schema for function 'output_formatter': ['first_name', 'last_name'] is not of type 'object', 'boolean'"]}]},{"cell_type":"code","source":[],"metadata":{"id":"DjtxvWZmGxjq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"OMzE3nfaGxm0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# MathChain\n","\n","It allows you to ask math questions and get back the correct results."],"metadata":{"id":"UuJyQazDGwxz"}},{"cell_type":"markdown","source":["Let's imagine we are creating a bot to help out students learn mathematics."],"metadata":{"id":"8U9lYk7sHB3e"}},{"cell_type":"code","source":["from langchain.chat_models import ChatOpenAI\n","from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate"],"metadata":{"id":"1I3am5IezGGJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langchain.schema import HumanMessage"],"metadata":{"id":"EJE6EqwtzGIS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = ChatOpenAI(openai_api_key=api_key)"],"metadata":{"id":"IR3CBDO1voje"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["result =  model([HumanMessage(content='What is 2 + 2')])"],"metadata":{"id":"W7ibZsNHvoor"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["result.content"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"j7v-pTHBHfEi","executionInfo":{"status":"ok","timestamp":1692017809495,"user_tz":-540,"elapsed":469,"user":{"displayName":"Gaurav Dahal","userId":"01909755880149023152"}},"outputId":"7b9ab8ad-40f6-4aff-80fb-a03da4c39a08"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'2 + 2 equals 4.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["17 ** 11"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_Cxc7f19HfIZ","executionInfo":{"status":"ok","timestamp":1692017831290,"user_tz":-540,"elapsed":604,"user":{"displayName":"Gaurav Dahal","userId":"01909755880149023152"}},"outputId":"50ee0acc-d205-4c2d-8930-4f343512b6bf"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["34271896307633"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["result =  model([HumanMessage(content='What is 17 raised to the power of 11?')])"],"metadata":{"id":"NeuxkOUvHfP3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["result.content"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":53},"id":"28so_ncGHfTh","executionInfo":{"status":"ok","timestamp":1692017860949,"user_tz":-540,"elapsed":322,"user":{"displayName":"Gaurav Dahal","userId":"01909755880149023152"}},"outputId":"acaa89cc-c1e3-4f28-828d-00649e04f789"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'17 raised to the power of 11 is equal to 78,664,062,500,000,000,000,000,000,000,000,000,000,000.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":13}]},{"cell_type":"markdown","source":["In the above result, you saw that our model started hallucinating the worng answer.\n","\n","`Note:` Keep in mind that at a certain point as the model's behind the scenes evaluation may get better and better for these sort of question, it may no longer be necessary.\n","\n","-----\n","\n","But the question becomes, how can I utilize a chat model that does hallucinations in order to get back mathematically correct answers?\n","\n","`Answer: We can actually evaluate string equations within.`"],"metadata":{"id":"xlU9WuZAH973"}},{"cell_type":"code","source":["eval(\"17**11\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lNPZiF4CHfW7","executionInfo":{"status":"ok","timestamp":1692018253730,"user_tz":-540,"elapsed":498,"user":{"displayName":"Gaurav Dahal","userId":"01909755880149023152"}},"outputId":"72117cd4-ddd5-48eb-d02b-5da541c89c16"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["34271896307633"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["# Luckily Langchain has already created this sort of functionality for us.\n","from langchain import LLMMathChain"],"metadata":{"id":"mEfe7p8ZHfaZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["llm_math_model = LLMMathChain.from_llm(model)"],"metadata":{"id":"PZipACieHfuT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Basically what this does internally is when it asks a question to chat GPT or any other AI model, like \"What is 17 raise to the power of 11?\"\n","\n","- It also asks it to format it just as a single string that can then be interpreted.\n","\n","- And then Langchain is going to run eval under the hood for us to get us the correct answer.\n","\n","- And it's going to output this as a dictionary."],"metadata":{"id":"_eic3glSJ8Ke"}},{"cell_type":"code","source":["llm_math_model(\"What is 17 raised to the power of 11?\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7AXvVlOHHfx-","executionInfo":{"status":"ok","timestamp":1692018650782,"user_tz":-540,"elapsed":1554,"user":{"displayName":"Gaurav Dahal","userId":"01909755880149023152"}},"outputId":"14a02e11-d7b9-448c-dd53-d72abf5ff3b0"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'question': 'What is 17 raised to the power of 11?',\n"," 'answer': 'Answer: 34271896307633'}"]},"metadata":{},"execution_count":17}]},{"cell_type":"markdown","source":["# AdditionalChains\n","\n","Let's take a look at using Q&A models for answering questions about a particular document."],"metadata":{"id":"yN_8X3VcLFGW"}},{"cell_type":"markdown","source":["This is a chain that's already pre-built for us that can easily connect to an existing Chroma vector store or  any other vector store that's open via langchain and then answer questions about the document.\n","\n","------\n","\n","### How to use pre-built chains?\n","There are so many pre-built chains. We don't really have time to go over all of them.\n","\n","We definitely want to take a quick peek at the available chains that are pre-built into langchain before having to implement our own solution.\n","\n","https://python.langchain.com/docs/modules/chains/\n","\n","-----\n","\n","Now, let's take a look at one of these additional chains that allows us to directly answer questions. So this is known as the QAchain."],"metadata":{"id":"NvcpUwtiL4x_"}},{"cell_type":"code","source":["# Step 1: First I need to connect to open AI embeddings and some sort of Chroma vector store.\n","# Step 2:  And then we basically want to do an QA on that VECTOR store."],"metadata":{"id":"AMYua3gdHf1g","executionInfo":{"status":"ok","timestamp":1692074832317,"user_tz":-540,"elapsed":347,"user":{"displayName":"Gaurav Dahal","userId":"01909755880149023152"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["from langchain.embeddings import OpenAIEmbeddings\n","from langchain.vectorstores import Chroma"],"metadata":{"id":"WrkGl1c1LIgK","executionInfo":{"status":"ok","timestamp":1692075000262,"user_tz":-540,"elapsed":278,"user":{"displayName":"Gaurav Dahal","userId":"01909755880149023152"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["import os\n","os.environ[\"OPENAI_API_KEY\"] = api_key"],"metadata":{"id":"t5WX6VqzLIji","executionInfo":{"status":"ok","timestamp":1692075260607,"user_tz":-540,"elapsed":299,"user":{"displayName":"Gaurav Dahal","userId":"01909755880149023152"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["embedding_function = OpenAIEmbeddings()"],"metadata":{"id":"m2lYBMC2LIq6","executionInfo":{"status":"ok","timestamp":1692075270411,"user_tz":-540,"elapsed":308,"user":{"displayName":"Gaurav Dahal","userId":"01909755880149023152"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["# It was our vectorized format of the US Constitution\n","# So that connects to our existing persistent directory of Chroma\n","db_connection = Chroma(persist_directory=\"F://PG_DA//LangChain//LangChainNotebooks-UNZIP-ME//01-Data-Connections//US_Consitution\",\n","                       embedding_function = embedding_function)"],"metadata":{"id":"lF-uZNdWLIuf","executionInfo":{"status":"ok","timestamp":1692075519378,"user_tz":-540,"elapsed":379,"user":{"displayName":"Gaurav Dahal","userId":"01909755880149023152"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["from langchain.chains.question_answering import load_qa_chain"],"metadata":{"id":"hpFrsN_FLIyc","executionInfo":{"status":"ok","timestamp":1692075726343,"user_tz":-540,"elapsed":297,"user":{"displayName":"Gaurav Dahal","userId":"01909755880149023152"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["# This one just gives us a little bit more metadata when it gets returned.\n","from langchain.chains.qa_with_sources import load_qa_with_sources_chain"],"metadata":{"id":"7zCav-jnLI2f","executionInfo":{"status":"ok","timestamp":1692075874682,"user_tz":-540,"elapsed":272,"user":{"displayName":"Gaurav Dahal","userId":"01909755880149023152"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["# Now, we need some sort of chat model\n","from langchain.chat_models import ChatOpenAI"],"metadata":{"id":"HsRzC1RBLI5w","executionInfo":{"status":"ok","timestamp":1692075922050,"user_tz":-540,"elapsed":288,"user":{"displayName":"Gaurav Dahal","userId":"01909755880149023152"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["llm = ChatOpenAI(temperature=0)"],"metadata":{"id":"NDna2-p6LJXT","executionInfo":{"status":"ok","timestamp":1692075938699,"user_tz":-540,"elapsed":486,"user":{"displayName":"Gaurav Dahal","userId":"01909755880149023152"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["# first we will use load_qa_chain\n","chain = load_qa_chain(llm, chain_type='stuff') # 'stuff' basically means we are going to insert context into a prompt call.\n","# PROMPT --> \"stuffing in context\" --> CHROMA (grab later on from chroma store)"],"metadata":{"id":"SbJOcFp7ldmM","executionInfo":{"status":"ok","timestamp":1692076686057,"user_tz":-540,"elapsed":274,"user":{"displayName":"Gaurav Dahal","userId":"01909755880149023152"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["question = \"What is the 15th amendment?\""],"metadata":{"id":"zDEzYY5AldpK","executionInfo":{"status":"ok","timestamp":1692076694092,"user_tz":-540,"elapsed":355,"user":{"displayName":"Gaurav Dahal","userId":"01909755880149023152"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["docs = db_connection.similarity_search(question)"],"metadata":{"id":"b4ccIcWdldr9","executionInfo":{"status":"ok","timestamp":1692076697763,"user_tz":-540,"elapsed":461,"user":{"displayName":"Gaurav Dahal","userId":"01909755880149023152"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["#docs"],"metadata":{"id":"VyRKVSGLlduX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["chain.run(input_documents = docs, question = question)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":70},"id":"BAaX6zJxldz2","executionInfo":{"status":"ok","timestamp":1692076702175,"user_tz":-540,"elapsed":1851,"user":{"displayName":"Gaurav Dahal","userId":"01909755880149023152"}},"outputId":"7cc402bf-94d8-4e0d-87e3-2b5ec49c2e74"},"execution_count":25,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'The 15th Amendment to the United States Constitution was ratified in 1870. It prohibits the denial of voting rights to any citizen based on their race, color, or previous condition of servitude.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":25}]},{"cell_type":"code","source":["# now trial with \"load_qa_with_sources_chain\"\n","chain = load_qa_with_sources_chain(llm, chain_type='stuff')\n","chain.run(input_documents = docs, question = question)\n","# it'll also tell me the 'sources', which was some data."],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"89DnKoKVn9ww","executionInfo":{"status":"ok","timestamp":1692076728697,"user_tz":-540,"elapsed":2231,"user":{"displayName":"Gaurav Dahal","userId":"01909755880149023152"}},"outputId":"c877d7d4-244a-48ac-a337-0f979f66b378"},"execution_count":26,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"I don't know what the 15th amendment is.\\nSOURCES:\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":26}]},{"cell_type":"markdown","source":["Here, with just a few lines of code We've gone ahead and duplicated a lot of the work we had to do before by manually doing that stuffing of the context ourselves.Now we no longer need to do that.\n","\n","And with just a few simple steps, create our model, create the chain, ask our question, get the documents, and pass those in together.\n","\n","And we do that via the load_qa_chain. The load_qa_with_sources_chain is extremely simiar, except its goining to give us back some meta information as well about the documnets that we use for that."],"metadata":{"id":"lFdTn8JFnKwl"}},{"cell_type":"code","source":[],"metadata":{"id":"m1_MK4mhld2b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"--3CLwmcld5J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"i3Z7mg30ld7q"},"execution_count":null,"outputs":[]}]}