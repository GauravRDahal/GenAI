{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyP66hNg314pbP5PC5LFtkVo"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"nO2oiVV1zKwA"},"outputs":[],"source":["# https://python.langchain.com/docs/modules/data_connection/document_loaders/pdf"]},{"cell_type":"code","source":["!pip install langchain openai"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Jh4eJ4Wp1Rrk","executionInfo":{"status":"ok","timestamp":1691729758005,"user_tz":-540,"elapsed":10531,"user":{"displayName":"Gaurav Dahal","userId":"01909755880149023152"}},"outputId":"e8f18085-e218-47ad-b85d-7babe4df9f90"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting langchain\n","  Downloading langchain-0.0.261-py3-none-any.whl (1.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting openai\n","  Downloading openai-0.27.8-py3-none-any.whl (73 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n","Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.19)\n","Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.8.5)\n","Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.2)\n","Collecting dataclasses-json<0.6.0,>=0.5.7 (from langchain)\n","  Downloading dataclasses_json-0.5.14-py3-none-any.whl (26 kB)\n","Collecting langsmith<0.1.0,>=0.0.11 (from langchain)\n","  Downloading langsmith-0.0.21-py3-none-any.whl (32 kB)\n","Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.4)\n","Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.23.5)\n","Collecting openapi-schema-pydantic<2.0,>=1.2 (from langchain)\n","  Downloading openapi_schema_pydantic-1.2.4-py3-none-any.whl (90 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.12)\n","Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n","Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.65.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.2.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n","Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n","  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n","  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n","Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2,>=1->langchain) (4.7.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.7.22)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\n","Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (23.1)\n","Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain)\n","  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n","Installing collected packages: mypy-extensions, marshmallow, typing-inspect, openapi-schema-pydantic, langsmith, openai, dataclasses-json, langchain\n","Successfully installed dataclasses-json-0.5.14 langchain-0.0.261 langsmith-0.0.21 marshmallow-3.20.1 mypy-extensions-1.0.0 openai-0.27.8 openapi-schema-pydantic-1.2.4 typing-inspect-0.9.0\n"]}]},{"cell_type":"code","source":["from langchain.document_loaders import CSVLoader"],"metadata":{"id":"UfWG9_zs1Fsj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["loader = CSVLoader(\"/content/penguins.csv\") #you may need to put full file path"],"metadata":{"id":"SNDOij1t1Fur"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data = loader.load()"],"metadata":{"id":"tZ57Yp8p1FyX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["type(data)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-S-JYPt-1F08","executionInfo":{"status":"ok","timestamp":1691140726618,"user_tz":-540,"elapsed":18,"user":{"displayName":"Gaurav Dahal","userId":"01909755880149023152"}},"outputId":"315dba36-5887-4189-e9fe-0c52ce64b8cf"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["list"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["print(data[0].page_content)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bGBZE2mr1F4C","executionInfo":{"status":"ok","timestamp":1691140816533,"user_tz":-540,"elapsed":535,"user":{"displayName":"Gaurav Dahal","userId":"01909755880149023152"}},"outputId":"838b40d8-a377-4911-e26d-469f3a2a92f0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["species: Adelie\n","island: Torgersen\n","bill_length_mm: 39.1\n","bill_depth_mm: 18.7\n","flipper_length_mm: 181\n","body_mass_g: 3750\n","sex: MALE\n"]}]},{"cell_type":"code","source":["print(data[13].page_content)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BatRWhLl1F7D","executionInfo":{"status":"ok","timestamp":1691140824256,"user_tz":-540,"elapsed":13,"user":{"displayName":"Gaurav Dahal","userId":"01909755880149023152"}},"outputId":"2789da45-d926-413d-e4e9-6be2e66ff338"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["species: Adelie\n","island: Torgersen\n","bill_length_mm: 38.6\n","bill_depth_mm: 21.2\n","flipper_length_mm: 191\n","body_mass_g: 3800\n","sex: MALE\n"]}]},{"cell_type":"code","source":["print(data[13].metadata)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hJQPSIfX1F9j","executionInfo":{"status":"ok","timestamp":1691140849954,"user_tz":-540,"elapsed":10,"user":{"displayName":"Gaurav Dahal","userId":"01909755880149023152"}},"outputId":"50a0b1d2-81a5-4efe-e273-9109d59987fb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'source': '/content/penguins.csv', 'row': 13}\n"]}]},{"cell_type":"markdown","source":["## HTML Loader"],"metadata":{"id":"I5nQeGzV2jN2"}},{"cell_type":"code","source":["!pip install beautifulsoup4"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r_8IUAwF2ccd","executionInfo":{"status":"ok","timestamp":1691140942858,"user_tz":-540,"elapsed":4494,"user":{"displayName":"Gaurav Dahal","userId":"01909755880149023152"}},"outputId":"ad33de80-c61f-46d5-af6b-df0db553bec4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.11.2)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.4.1)\n"]}]},{"cell_type":"code","source":["from langchain.document_loaders import BSHTMLLoader"],"metadata":{"id":"d0Oo1pgq2cfL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["loader = BSHTMLLoader(\"/content/some_website.html\")"],"metadata":{"id":"RhePRwe32ciR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data = loader.load()"],"metadata":{"id":"j9hlmi652ckR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data[0].page_content"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"n-1hFqVB2coO","executionInfo":{"status":"ok","timestamp":1691141165079,"user_tz":-540,"elapsed":36,"user":{"displayName":"Gaurav Dahal","userId":"01909755880149023152"}},"outputId":"d6dfbc49-f1e9-42b4-8a9a-adb1c62fed0b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Heading 1'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":19}]},{"cell_type":"markdown","source":["# PYPDF loader"],"metadata":{"id":"LzZMtpNs3p9H"}},{"cell_type":"code","source":["!pip install pypdf"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p7k3J8BR2cqE","executionInfo":{"status":"ok","timestamp":1691141233246,"user_tz":-540,"elapsed":7029,"user":{"displayName":"Gaurav Dahal","userId":"01909755880149023152"}},"outputId":"705b0d75-b542-494b-eb38-c9f0ec3890a8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pypdf\n","  Downloading pypdf-3.14.0-py3-none-any.whl (269 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/269.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.4/269.8 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m269.8/269.8 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: pypdf\n","Successfully installed pypdf-3.14.0\n"]}]},{"cell_type":"code","source":["from langchain.document_loaders import PyPDFLoader"],"metadata":{"id":"OklOSCCn2c7Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["loader = PyPDFLoader(\"/content/SomeReport.pdf\")"],"metadata":{"id":"BoSThuEz2c9K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data = loader.load()"],"metadata":{"id":"BUIglenu2dC2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data[0].page_content.replace(\"\\n\",\" \")\n","# replacing \\n with a space"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":53},"id":"db3y00mX2dEq","executionInfo":{"status":"ok","timestamp":1691142037417,"user_tz":-540,"elapsed":494,"user":{"displayName":"Gaurav Dahal","userId":"01909755880149023152"}},"outputId":"c220fb29-80ad-49a9-b8ce-9a08dcf23855"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'This is the first line PDF. This is the second line in the PDF. This is the third line in the PDF.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":45}]},{"cell_type":"markdown","source":["My_trail"],"metadata":{"id":"QtBwvAUS4kup"}},{"cell_type":"code","source":["loader2 = PyPDFLoader(\"/content/Gaurav Raja Dahal Data Analyst Offer Letter.pdf\")"],"metadata":{"id":"4NsEiV0o1F_p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data2 = loader2.load()"],"metadata":{"id":"S9mkcOgF1GC4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data2[0].metadata"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z_RLwvle59so","executionInfo":{"status":"ok","timestamp":1691141856959,"user_tz":-540,"elapsed":469,"user":{"displayName":"Gaurav Dahal","userId":"01909755880149023152"}},"outputId":"52d1802c-f9b8-4030-9cf4-97a4009304eb"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'source': '/content/Gaurav Raja Dahal Data Analyst Offer Letter.pdf',\n"," 'page': 0}"]},"metadata":{},"execution_count":44}]},{"cell_type":"markdown","source":["# Document Loaders - Integrations"],"metadata":{"id":"gm8OoCCR7eGN"}},{"cell_type":"code","source":["# https://python.langchain.com/docs/integrations/document_loaders/hacker_news\n","\n","from langchain.document_loaders import HNLoader"],"metadata":{"id":"xKfGOBJT6RXA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["loader = HNLoader(\"https://news.ycombinator.com/item?id=36989798\")"],"metadata":{"id":"i_DHtK1J6RZv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data  = loader.load()"],"metadata":{"id":"nY3M3Rsf6Rct"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(data[0].page_content)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0tFit5kC6Re_","executionInfo":{"status":"ok","timestamp":1691717868260,"user_tz":-540,"elapsed":304,"user":{"displayName":"Gaurav Dahal","userId":"01909755880149023152"}},"outputId":"fe2074fa-b95c-4fc1-bbc9-0104c73df1f5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["amluto 7 days ago  \n","             | next [–] \n","\n","> A new blog post shows you how to use Elastic Load Balancers and NAT Gateways for ingress and egress traffic, while avoiding the use of a public IPv4 address for each instance that you launch.It would be nice if this came with reasonably priced NAT gateways.  The current pricing is outrageous.\n"," \n","reply\n"]}]},{"cell_type":"code","source":["from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate"],"metadata":{"id":"dbgRG2zY6Riv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langchain.chat_models import ChatOpenAI\n","f = open(\"/content/api_key.txt\")\n","api_key = f.read()\n","model = ChatOpenAI(openai_api_key=api_key)"],"metadata":{"id":"KtEjc9rF6Ryy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["human_prompt = HumanMessagePromptTemplate.from_template(\"Please give me a short summary of the following HackerNews comment:\\n{comment}\")"],"metadata":{"id":"vVWaZC1u6R04"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["chat_prompt = ChatPromptTemplate.from_messages([human_prompt])"],"metadata":{"id":"LgpNb-rp-YsH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["result = model(chat_prompt.format_prompt(comment = data[0].page_content).to_messages())"],"metadata":{"id":"zAXeHY_A-YvF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["result.content"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":105},"id":"_WtCFyfT-Yx-","executionInfo":{"status":"ok","timestamp":1691717906193,"user_tz":-540,"elapsed":785,"user":{"displayName":"Gaurav Dahal","userId":"01909755880149023152"}},"outputId":"ea48d0fb-839e-48e4-d700-038c945e84ed"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'In this HackerNews comment, the user mentions a new blog post that explains how to use Elastic Load Balancers and NAT Gateways for managing incoming and outgoing network traffic without requiring a public IPv4 address for each instance. However, the user also expresses dissatisfaction with the current pricing of NAT gateways, considering it to be too expensive.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":11}]},{"cell_type":"markdown","source":["# Document Transformers\n"],"metadata":{"id":"nJeJ5WLFPXXo"}},{"cell_type":"code","source":["from langchain.text_splitter import CharacterTextSplitter"],"metadata":{"id":"LmOsK_ARg-ZT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with open(\"/content/FDR_State_of_Union_1944.txt\") as file:\n","  speech_text = file.read()"],"metadata":{"id":"7hZDs_KWg-bz","colab":{"base_uri":"https://localhost:8080/","height":210},"executionInfo":{"status":"error","timestamp":1691672645656,"user_tz":-540,"elapsed":20,"user":{"displayName":"Gaurav Dahal","userId":"01909755880149023152"}},"outputId":"da8dc529-9ecb-46b0-cdd8-2b3080ac4c03"},"execution_count":null,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-83c5a5308f33>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/FDR_State_of_Union_1944.txt\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m   \u001b[0mspeech_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/FDR_State_of_Union_1944.txt'"]}]},{"cell_type":"code","source":["len(speech_text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2uhEXzEhg-mD","executionInfo":{"status":"ok","timestamp":1691202887906,"user_tz":-540,"elapsed":6,"user":{"displayName":"Gaurav Dahal","userId":"01909755880149023152"}},"outputId":"6f89c4fd-a1a0-41f1-b342-2b9a82a16f5a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["21927"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["len(speech_text.split())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QG74riDDg-oj","executionInfo":{"status":"ok","timestamp":1691202905614,"user_tz":-540,"elapsed":322,"user":{"displayName":"Gaurav Dahal","userId":"01909755880149023152"}},"outputId":"539f91db-507d-413b-f123-8f75999f998c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["3750"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["text_splitter = CharacterTextSplitter(separator=\"\\n\\n\", chunk_size = 1000)"],"metadata":{"id":"nIKbU-xKg-xJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# creating new object\n","texts = text_splitter.create_documents([speech_text])"],"metadata":{"id":"XnPz7pxAg-0U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# texts[0].page_content"],"metadata":{"id":"hLKmyR9Ig_Bs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install tiktoken # converts words to tokens\n","# I can use the tick token library directly with character splitter to actually encode based off the size of the tokens."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FAQDju_fg_Ec","executionInfo":{"status":"ok","timestamp":1691204507967,"user_tz":-540,"elapsed":6225,"user":{"displayName":"Gaurav Dahal","userId":"01909755880149023152"}},"outputId":"4e6066fd-65db-4641-9920-c99860b42174"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting tiktoken\n","  Downloading tiktoken-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2022.10.31)\n","Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.27.1)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2023.7.22)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4)\n","Installing collected packages: tiktoken\n","Successfully installed tiktoken-0.4.0\n"]}]},{"cell_type":"code","source":["text_splitter = CharacterTextSplitter.from_tiktoken_encoder(chunk_size = 500)\n","# You can also pass in a tokenizer from Huggingface if you're using a hugging face, open source embedding model."],"metadata":{"id":"Idkf2cydg_H3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["texts = text_splitter.split_text(speech_text)"],"metadata":{"id":"I97joupVg_LL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(texts)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a3YVeRuLg_OL","executionInfo":{"status":"ok","timestamp":1691204816651,"user_tz":-540,"elapsed":15,"user":{"displayName":"Gaurav Dahal","userId":"01909755880149023152"}},"outputId":"39972c68-6a48-4cb2-d6e2-82283d372c47"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["15"]},"metadata":{},"execution_count":17}]},{"cell_type":"markdown","source":["#Text Embeddings with Langchain\n","\n","We will have these embeddings stored in memory RAM as a python list.\n"],"metadata":{"id":"p5nu2I3EqpLM"}},{"cell_type":"code","source":["from langchain.embeddings import OpenAIEmbeddings"],"metadata":{"id":"8LgCiT4H-Yz7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","os.environ[\"OPENAI_API_KEY\"] = api_key"],"metadata":{"id":"nEDCIaLb6R5G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["embeddings = OpenAIEmbeddings()"],"metadata":{"id":"xsXK0xhJJDbR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# embedding a string\n","text = \"this is some normal text string that  I want to embedd as a vector.\""],"metadata":{"id":"mIpog1aKJDdq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["embedded_text = embeddings.embed_query(text) # this cost us some amout of money"],"metadata":{"id":"8A796XolJDgr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# embedded_text"],"metadata":{"id":"hRtHoazfJDjy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# If we want to embed a document\n","from langchain.document_loaders import CSVLoader"],"metadata":{"id":"-sBTpNVgJDmd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["loader = CSVLoader(\"/content/penguins.csv\")"],"metadata":{"id":"oh_brqghJDo3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data = loader.load()"],"metadata":{"id":"y8sxzEYfJD6x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# [text.page_content for text in data]"],"metadata":{"id":"PgI_ShRZJD9I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### NOTE: This is going to cost a lot.###\n","### Becareful running this line ###\n","\n","# embedded_docs = embeddings.embed_documents(text.page_content for text in data)"],"metadata":{"id":"CR2uRA-OLGP9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Vector Store\n","\n","Here, we want to be able to shut down our computer and still have access to those embeddings.\n","\n","Key attributes of Vector Store (vector database):\n","- can store large N-dimensional vectors.\n","- can directly index an embedded vector to its associated string text document.\n","- Can be \"queried\", allowing for a cosine similarity search between a new vector that's not in the database and the stored vectors\n","- can easily add, update, or delete new vectors like any other database\n","\n"],"metadata":{"id":"D85oI7kCLkIE"}},{"cell_type":"markdown","source":["### *Here, We will use an open-source and free vector store called Chroma, which has great integrations with Langchain.*"],"metadata":{"id":"fjfGmYG9OAfD"}},{"cell_type":"code","source":["!pip install chromadb\n","import chromadb\n","#open-source and free vector store\n","\n","# https://python.langchain.com/docs/integrations/vectorstores/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VPbdNq5nLGSd","executionInfo":{"status":"ok","timestamp":1692626868855,"user_tz":-540,"elapsed":4651,"user":{"displayName":"Gaurav Dahal","userId":"01909755880149023152"}},"outputId":"5ce0020a-017a-438b-ca00-a6d58103d54f"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: chromadb in /usr/local/lib/python3.10/dist-packages (0.4.6)\n","Requirement already satisfied: requests>=2.28 in /usr/local/lib/python3.10/dist-packages (from chromadb) (2.31.0)\n","Requirement already satisfied: pydantic<2.0,>=1.9 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.10.12)\n","Requirement already satisfied: chroma-hnswlib==0.7.2 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.7.2)\n","Requirement already satisfied: fastapi<0.100.0,>=0.95.2 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.99.1)\n","Requirement already satisfied: uvicorn[standard]>=0.18.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.23.2)\n","Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.23.5)\n","Requirement already satisfied: posthog>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (3.0.2)\n","Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.7.1)\n","Requirement already satisfied: pulsar-client>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (3.2.0)\n","Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.15.1)\n","Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.13.3)\n","Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.48.9)\n","Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.66.1)\n","Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (7.4.0)\n","Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.0.1)\n","Requirement already satisfied: starlette<0.28.0,>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from fastapi<0.100.0,>=0.95.2->chromadb) (0.27.0)\n","Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n","Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (23.5.26)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (23.1)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (3.20.3)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.12)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb) (1.16.0)\n","Requirement already satisfied: monotonic>=1.5 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb) (1.6)\n","Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n","Requirement already satisfied: python-dateutil>2.1 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb) (2.8.2)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from pulsar-client>=3.1.0->chromadb) (2023.7.22)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->chromadb) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->chromadb) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->chromadb) (2.0.4)\n","Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (8.1.6)\n","Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.14.0)\n","Requirement already satisfied: httptools>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.0)\n","Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (6.0.1)\n","Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.17.0)\n","Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.19.0)\n","Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (11.0.3)\n","Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette<0.28.0,>=0.27.0->fastapi<0.100.0,>=0.95.2->chromadb) (3.7.1)\n","Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi<0.100.0,>=0.95.2->chromadb) (1.3.0)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi<0.100.0,>=0.95.2->chromadb) (1.1.3)\n"]}]},{"cell_type":"code","source":["from langchain.embeddings import OpenAIEmbeddings"],"metadata":{"id":"0C_PpvEALGVL","colab":{"base_uri":"https://localhost:8080/","height":314},"executionInfo":{"status":"error","timestamp":1692626262102,"user_tz":-540,"elapsed":475,"user":{"displayName":"Gaurav Dahal","userId":"01909755880149023152"}},"outputId":"400f08b6-3cc9-4904-d9f8-7908da81d112"},"execution_count":1,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-ae51ee620ad5>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOpenAIEmbeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'langchain'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]},{"cell_type":"code","source":["# If we have a large document we need to split it\n","from langchain.text_splitter import CharacterTextSplitter"],"metadata":{"id":"sEZD0CiwLGXU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langchain.vectorstores import  Chroma"],"metadata":{"id":"63QEFXOdLGaG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langchain.document_loaders import TextLoader"],"metadata":{"id":"rgiH8G_BLGcA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Process for a vector store\n","- loading a document,\n","- splitting it up,\n","- embedding it, and\n","- then saving those embeddings to disk using something like Chroma."],"metadata":{"id":"EDTm2hjTe_S4"}},{"cell_type":"code","source":["# Load Document --> split into chunks\n","\n","# use embedding model --> embed chunks --> vectors\n","\n","# vector chunks --> save chromaDB\n","\n","# \"query\" --> similarity search chromadb"],"metadata":{"id":"r0s_0938LGe3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install tiktoken\n","import tiktoken"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JqmW_00_g2tp","executionInfo":{"status":"ok","timestamp":1691730810411,"user_tz":-540,"elapsed":8375,"user":{"displayName":"Gaurav Dahal","userId":"01909755880149023152"}},"outputId":"aab0a357-cd39-4651-e23e-0a4ff9677e26"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting tiktoken\n","  Downloading tiktoken-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.7 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.1/1.7 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.8/1.7 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2022.10.31)\n","Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2023.7.22)\n","Installing collected packages: tiktoken\n","Successfully installed tiktoken-0.4.0\n"]}]},{"cell_type":"code","source":["loader = TextLoader(\"/content/FDR_State_of_Union_1944.txt\")\n","documents = loader.load()\n","text_splitter = CharacterTextSplitter.from_tiktoken_encoder(chunk_size = 500)\n","docs = text_splitter.split_documents(documents)"],"metadata":{"id":"yEPwu0UyLGgq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# docs"],"metadata":{"id":"SJYO9qtjLGks"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["embedding_function = OpenAIEmbeddings()"],"metadata":{"id":"ynSAaexMJD_z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# We're going to  use Chroma to pass both the embeddings and the documentation into its database.\n","db = Chroma.from_documents(docs, embedding_function, persist_directory=\"./speech_new_db\")"],"metadata":{"id":"ZKiGG5A4JEC5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["It grabbed my documents which remember is a list of these documents and it basically went for every single one, grabbed the page content and then it sent it to OpenAI to be embedded.\n","\n","And now it has the document and its associated vector embedding."],"metadata":{"id":"8JohXzZSiXkv"}},{"cell_type":"code","source":["# DB persist in order to force the save to your actual computer or whateveryou're working with.\n","db.persist()"],"metadata":{"id":"J3uruAgERTCC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["If I do expand this folder for my file browser, I should now see speech new DB and if I double click on this I should see both the chrome collections.\n","\n","Those are your actual documents,\n","- the strings and\n","- then chrome embeddings.Those are the vectors.\n","\n","And then you have an index that essentially connects the two and then allows us to do things like similarity searches or lookups."],"metadata":{"id":"nTFADHrTjq-9"}},{"cell_type":"code","source":["# Now, after we shut down our computer and we want to reload our vector\n","db_new_connection = Chroma(persist_directory = '/content/speech_new_db',\n","                           embedding_function=embedding_function)"],"metadata":{"id":"suRJSSM-RTEw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Above, In order to go from new string to embedding, even if the DB connection is already saved, I still need to tell it, \"What embedding function to use.\"\n","\n","And that's kind of just a long winded way of saying that you do need to re inform it of your embedding function.\n","\n","So we'll say **\"embedding function = embedding function\"** we had before.\n","\n","This allows it to do things like similarity searches."],"metadata":{"id":"V9ghjHo0lBew"}},{"cell_type":"code","source":["# inside Chroma --> text str / (1,2,3)\n","# new str --> [3,4,5] --> similarity search in chroma"],"metadata":{"id":"rdU1Ldlyl-I8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["So I have this giant FDR speech.\n","\n","I want to be able to do something like figure out what did FDR say about a particular food law."],"metadata":{"id":"KtnP_zUKl-dC"}},{"cell_type":"code","source":["new_doc = \"What did FDR say about the cost of food law?\"\n","# all Chroma is going to do is vectorize this\n","# and then return the document that is most similar to the text inside this question."],"metadata":{"id":"MCNncvJYRTHo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We're doing a similarity search, it's just looking at how similar is the text here versus what's already in my database."],"metadata":{"id":"Aqe7Kyfjmpyu"}},{"cell_type":"code","source":["# So now I have a string\n","# How do I get the most similar documents?\n","# --> Well, I can say now similar docs is equal to my DB, either DB or DB.\n","similar_docs = db_new_connection.similarity_search(new_doc)"],"metadata":{"id":"cjlp8G3YRTKO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["similar_docs"],"metadata":{"id":"G-LWF25wRTNJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(similar_docs[0].page_content)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qaGT5zV1n-b0","executionInfo":{"status":"ok","timestamp":1691590097999,"user_tz":-540,"elapsed":7,"user":{"displayName":"Gaurav Dahal","userId":"01909755880149023152"}},"outputId":"8db08063-e848-4a47-8969-783bba3c4941"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["That is the way to fight and win a war—all out—and not with half-an-eye on the battlefronts abroad and the other eye-and-a-half on personal, selfish, or political interests here at home.\n","\n","Therefore, in order to concentrate all our energies and resources on winning the war, and to maintain a fair and stable economy at home, I recommend that the Congress adopt:\n","\n","(1) A realistic tax law—which will tax all unreasonable profits, both individual and corporate, and reduce the ultimate cost of the war to our sons and daughters. The tax bill now under consideration by the Congress does not begin to meet this test.\n","\n","(2) A continuation of the law for the renegotiation of war contracts—which will prevent exorbitant profits and assure fair prices to the Government. For two long years I have pleaded with the Congress to take undue profits out of war.\n","\n","(3) A cost of food law—which will enable the Government (a) to place a reasonable floor under the prices the farmer may expect for his production; and (b) to place a ceiling on the prices a consumer will have to pay for the food he buys. This should apply to necessities only; and will require public funds to carry out. It will cost in appropriations about one percent of the present annual cost of the war.\n","\n","(4) Early reenactment of. the stabilization statute of October, 1942. This expires June 30, 1944, and if it is not extended well in advance, the country might just as well expect price chaos by summer.\n","\n","(5) A national service law- which, for the duration of the war, will prevent strikes, and, with certain appropriate exceptions, will make available for war production or for any other essential services every able-bodied adult in this Nation.\n","\n","These five measures together form a just and equitable whole. I would not recommend a national service law unless the other laws were passed to keep down the cost of living, to share equitably the burdens of taxation, to hold the stabilization line, and to prevent undue profits.\n","\n","The Federal Government already has the basic power to draft capital and property of all kinds for war purposes on a basis of just compensation.\n"]}]},{"cell_type":"markdown","source":["Now what Croma has done is it grabbed the most similar_docs.\n","\n","And keep in mind you can specify a certain amount.\n","\n","So by default it gives you the 4 most similar and it comes in that order of similarity, most similar to least similar.\n","\n","Sometimes maybe you just want one so you can just grab the first one. It's up to you if you want to do it within the k parameter here or just by saying 0, and that gives you the most similar documentation."],"metadata":{"id":"X1Q0ait5nm3q"}},{"cell_type":"markdown","source":["## Lets imagine I wanted to add a new document to this existing chroma DB"],"metadata":{"id":"uPIYFw6WojuD"}},{"cell_type":"code","source":["\n","loader = TextLoader(\"/content/Lincoln_State_of_Union_1862.txt\")\n","documents = loader.load()"],"metadata":{"id":"uvooiMjiRTTf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["docs = text_splitter.split_documents(documents)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GDKHTHoVovvX","executionInfo":{"status":"ok","timestamp":1691718725007,"user_tz":-540,"elapsed":294,"user":{"displayName":"Gaurav Dahal","userId":"01909755880149023152"}},"outputId":"5646abf3-84d6-43c3-d0d0-0eaeb5738e93"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:langchain.text_splitter:Created a chunk of size 608, which is longer than the specified 500\n","WARNING:langchain.text_splitter:Created a chunk of size 539, which is longer than the specified 500\n","WARNING:langchain.text_splitter:Created a chunk of size 686, which is longer than the specified 500\n"]}]},{"cell_type":"code","source":["# How do I put them into the directory?\n","#--> I just load them in.\n","db_new_connection = Chroma.from_documents(docs, embedding_function,\n","                                          persist_directory=\"/content/speech_new_db\")"],"metadata":{"id":"aPKFPaU4ovyr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["docs = db_new_connection.similarity_search(\"slavery\")"],"metadata":{"id":"bRWnV17Vov2p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["docs[0].page_content"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":244},"id":"ZkDNlgFfov6B","executionInfo":{"status":"ok","timestamp":1691718746962,"user_tz":-540,"elapsed":301,"user":{"displayName":"Gaurav Dahal","userId":"01909755880149023152"}},"outputId":"4cdd4ecd-476b-42c0-ef5a-ce16ddd7f968"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'As to the second article, I think it would be impracticable to return to bondage the class of persons therein contemplated. Some of them, doubtless, in the property sense belong to loyal owners, and hence provision is made in this article for compensating such. The third article relates to the future of the freed people. It does not oblige, but merely authorizes Congress to aid in colonizing such as may consent. This ought not to be regarded as objectionable on the one hand or on the other, insomuch as it comes to nothing unless by the mutual consent of the people to be deported and the American voters, through their representatives in Congress.\\n\\nI can not make it better known than it already is that I strongly favor colonization; and yet I wish to say there is an objection urged against free colored persons remaining in the country which is largely imaginary, if not sometimes malicious.\\n\\nIt is insisted that their presence would injure and displace white labor and white laborers. If there ever could be a proper time for mere catch arguments, that time surely is not now. In times like the present men should utter nothing for which they would not willingly be responsible through time and in eternity. Is it true, then, that colored people can displace any more white labor by being free than by remaining slaves? If they stay in their old places, they jostle no white laborers; if they leave their old places, they leave them open to white laborers. Logically, there is neither more nor less of it. Emancipation, even without deportation, would probably enhance the wages of white labor, and very surely would not reduce them. Thus the customary amount of labor would still have to be performed—the freed people would surely not do more than their old proportion of it, and very probably for a time would do less, leaving an increased part to white laborers, bringing their labor into greater demand, and consequently enhancing the wages of it. With deportation, even to a limited extent, enhanced wages to white labor is mathematically certain. Labor is like any other commodity in the market—increase the demand for it and you increase the price of it. Reduce the supply of black labor by colonizing the black laborer out of the country, and by precisely so much you increase the demand for and wages of white labor.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":36}]},{"cell_type":"code","source":["docs = db_new_connection.similarity_search(\"cost of food law\")"],"metadata":{"id":"F4r7QUYvov-Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["docs[0].page_content"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":244},"id":"Nzz9dXy8owCV","executionInfo":{"status":"ok","timestamp":1691718759462,"user_tz":-540,"elapsed":300,"user":{"displayName":"Gaurav Dahal","userId":"01909755880149023152"}},"outputId":"d6237e5c-f8e3-4931-866b-2190c82ed781"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'That is the way to fight and win a war—all out—and not with half-an-eye on the battlefronts abroad and the other eye-and-a-half on personal, selfish, or political interests here at home.\\n\\nTherefore, in order to concentrate all our energies and resources on winning the war, and to maintain a fair and stable economy at home, I recommend that the Congress adopt:\\n\\n(1) A realistic tax law—which will tax all unreasonable profits, both individual and corporate, and reduce the ultimate cost of the war to our sons and daughters. The tax bill now under consideration by the Congress does not begin to meet this test.\\n\\n(2) A continuation of the law for the renegotiation of war contracts—which will prevent exorbitant profits and assure fair prices to the Government. For two long years I have pleaded with the Congress to take undue profits out of war.\\n\\n(3) A cost of food law—which will enable the Government (a) to place a reasonable floor under the prices the farmer may expect for his production; and (b) to place a ceiling on the prices a consumer will have to pay for the food he buys. This should apply to necessities only; and will require public funds to carry out. It will cost in appropriations about one percent of the present annual cost of the war.\\n\\n(4) Early reenactment of. the stabilization statute of October, 1942. This expires June 30, 1944, and if it is not extended well in advance, the country might just as well expect price chaos by summer.\\n\\n(5) A national service law- which, for the duration of the war, will prevent strikes, and, with certain appropriate exceptions, will make available for war production or for any other essential services every able-bodied adult in this Nation.\\n\\nThese five measures together form a just and equitable whole. I would not recommend a national service law unless the other laws were passed to keep down the cost of living, to share equitably the burdens of taxation, to hold the stabilization line, and to prevent undue profits.\\n\\nThe Federal Government already has the basic power to draft capital and property of all kinds for war purposes on a basis of just compensation.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":38}]},{"cell_type":"code","source":["docs[0].metadata"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3eBS-IGLS8YL","executionInfo":{"status":"ok","timestamp":1691718780195,"user_tz":-540,"elapsed":313,"user":{"displayName":"Gaurav Dahal","userId":"01909755880149023152"}},"outputId":"e2220494-9eaa-46a2-857d-4b77e89643d2"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'source': '/content/FDR_State_of_Union_1944.txt'}"]},"metadata":{},"execution_count":39}]},{"cell_type":"markdown","source":["# Vector Store Retriever"],"metadata":{"id":"2OW-DJ8uR2mE"}},{"cell_type":"code","source":["type(db_new_connection)"],"metadata":{"id":"Gm3H7AeZowFS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1691718807593,"user_tz":-540,"elapsed":398,"user":{"displayName":"Gaurav Dahal","userId":"01909755880149023152"}},"outputId":"11b3339b-96f8-4d05-8795-581c18387b55"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["langchain.vectorstores.chroma.Chroma"]},"metadata":{},"execution_count":40}]},{"cell_type":"markdown","source":["When we make these connections, we can call the \"as_retriever()\" method and that creates what's known as a retriever object."],"metadata":{"id":"v-NCTbW4TUNX"}},{"cell_type":"code","source":["retriever = db_new_connection.as_retriever()"],"metadata":{"id":"MAtmMNdOowOb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["result = retriever.get_relevant_documents(\"cost of food law\")"],"metadata":{"id":"KUpKV-EgowWJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# result"],"metadata":{"id":"CFzlh5k4UJhA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Here, we can also give it things like a score threshold, but just know that:\n","- you can take any chrome or DB vector store connection,\n","- convert it to a retriever, and\n","- then another set of methods is available.\n","\n","Why this is important to know?\n","--\n","Because when we learn about things like multi query retrieval or sometimes context compression, we actually pass in a retriever object rather than a vector store object."],"metadata":{"id":"eHkqPjHGUpor"}},{"cell_type":"markdown","source":["## Multi-Query Retriever\n","\n","Sometimes the documents in your vector store may contain phrasing that you're not really aware of due to the size of the overall vector store.\n","\n","Maybe you have thousands of documents in there and what you're looking for isn't actually quite phrased the way you're thinking about it in your initial query for a similarity search.\n","\n","This can then cause issues in trying to think of that correct query string for similarity comparisons.\n","\n","### A solution we could try is to use a large language model, like a chat model to generate multiple variations of our query using ***multi query retriever***, allowing us to focus on key ideas rather than the exact phrasing."],"metadata":{"id":"VO4sTVzaVU6V"}},{"cell_type":"markdown","source":["Steps:\n","1. We are going to ask a question\n","2. Our model is going to make a couple of variations of that initial query/question\n","3. Then it's going to use those to retrieve the relevant documents"],"metadata":{"id":"qLFrlhtMWm0m"}},{"cell_type":"code","source":["!pip install wikipedia"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yGZYObTWYKPL","executionInfo":{"status":"ok","timestamp":1691730721963,"user_tz":-540,"elapsed":7852,"user":{"displayName":"Gaurav Dahal","userId":"01909755880149023152"}},"outputId":"3925e300-a774-446d-a93b-bd22341c1202"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting wikipedia\n","  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (4.11.2)\n","Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (2.31.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2023.7.22)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->wikipedia) (2.4.1)\n","Building wheels for collected packages: wikipedia\n","  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11680 sha256=5af457d5ff33ffd762290e64df54ba97cbb5fd538389cb6cb73c444c720d3302\n","  Stored in directory: /root/.cache/pip/wheels/5e/b6/c5/93f3dec388ae76edc830cb42901bb0232504dfc0df02fc50de\n","Successfully built wikipedia\n","Installing collected packages: wikipedia\n","Successfully installed wikipedia-1.4.0\n"]}]},{"cell_type":"code","source":["from langchain.document_loaders import WikipediaLoader"],"metadata":{"id":"iXd1cdUbUJkB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["loader = WikipediaLoader(query = \"MKUltra\")\n","documents = loader.load()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Kf_Ux8tnUJnM","executionInfo":{"status":"ok","timestamp":1691730742147,"user_tz":-540,"elapsed":10284,"user":{"displayName":"Gaurav Dahal","userId":"01909755880149023152"}},"outputId":"79430b40-c308-4961-f24e-6b0bdf808439"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/wikipedia/wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n","\n","The code that caused this warning is on line 389 of the file /usr/local/lib/python3.10/dist-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n","\n","  lis = BeautifulSoup(html).find_all('li')\n"]}]},{"cell_type":"code","source":["# this is the entire wikipedia page so its very long\n","# documents\n","len(documents)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HFKChlwoUJpn","executionInfo":{"status":"ok","timestamp":1691730760543,"user_tz":-540,"elapsed":335,"user":{"displayName":"Gaurav Dahal","userId":"01909755880149023152"}},"outputId":"fb9bed0f-9b15-4cdc-c9c1-ab2dac3cd76d"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["9"]},"metadata":{},"execution_count":8}]},{"cell_type":"markdown","source":["Here, we can see here I have 9 documents there.\n","\n","Chunk size is probably really large, so I'm going to make this smaller using some splitting.\n","\n","So lets do a document transformer here.\n","(In this case we will do the character text splitter)"],"metadata":{"id":"qfbBeRjrYyoa"}},{"cell_type":"code","source":["from langchain.text_splitter import CharacterTextSplitter"],"metadata":{"id":"GBbjTZEwUJvl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["text_splitter = CharacterTextSplitter.from_tiktoken_encoder(chunk_size = 500)\n","docs = text_splitter.split_documents(documents)"],"metadata":{"id":"eGPyb_EwUJxz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(docs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tapl5vbDUJz_","executionInfo":{"status":"ok","timestamp":1691730831870,"user_tz":-540,"elapsed":299,"user":{"displayName":"Gaurav Dahal","userId":"01909755880149023152"}},"outputId":"f0c6bcaf-e2cd-4454-8087-2a99bc9cfd03"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["19"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["# next, we will create an embedding function\n","from langchain.embeddings import OpenAIEmbeddings"],"metadata":{"id":"_70AlVIwUJ2P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["embedding_function = OpenAIEmbeddings()"],"metadata":{"id":"3RaUQ0rIUJ4r"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Next, we will embed the documents to ChromaDB\n","from langchain.vectorstores import Chroma"],"metadata":{"id":"n2mgXRwjUJ8w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# From documents pass in those docs we just made that now are in smaller chunk sizes (18)\n","# Pass in the embedding function we just made\n","# And then let's make a new directory for persistence\n","db = Chroma.from_documents(docs, embedding_function,\n","                           persist_directory = \"./some_new_mkultra\")\n","# Here, this will essentially just be the Wikipedia page for MKUltra."],"metadata":{"id":"bdHl7xvtaSBP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["db.persist()\n","# And what that did is it used what I just downloaded from Wikipedia about the MKUltra program, sent it over to OpenAI.\n","# It's vectorized now and then it saved it locally to my computer to some_new_mkultra."],"metadata":{"id":"-wPZZ77oaSFA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now, let's understand how to use the multi-query\n","\n","So what I'm going to do is actually a couple more imports.\n","- One is the multi query retriever, which is going to allow us to split up a query into variations and\n","- then the chat model, which is actually going to create those variations for us"],"metadata":{"id":"gDjabQM7b7yL"}},{"cell_type":"code","source":["from langchain.retrievers.multi_query import MultiQueryRetriever\n","from langchain.chat_models import ChatOpenAI"],"metadata":{"id":"hl9baAWsaSIN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### So let's imagine my initial question.\n","\n","***I want to know just when was this declassified?***\n","--\n","Here, I'm going to get back is the relevant page content from the Wikipedia article.\n","\n","I am not actually going to get the language model to answer this question, at least not yet."],"metadata":{"id":"ftvhFH_ecu56"}},{"cell_type":"code","source":["question = \"When was this declassified?\""],"metadata":{"id":"3MZWrCmraSLc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["llm = ChatOpenAI(temperature=0)"],"metadata":{"id":"kVuveHd5aSPr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Here, I'm going to say retriever is equal to my database connection from earlier, which was db up smewhere.\n","# And then convert that to be as a retriever, as we discussed previously.\n","# And then, I just need to pass in the llm I'm using, which is the chat model.\n","retriever_from_llm = MultiQueryRetriever.from_llm(retriever=db.as_retriever(),\n","                                                  llm=llm)"],"metadata":{"id":"ch2hNO3baSTF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["-----------------------"],"metadata":{"id":"HOlM0UnAguh3"}},{"cell_type":"markdown","source":["## Logging Behind Scenes\n","\n","What I'm going to do is just show you a little bit of logging that occurs behind the scenes so you can understand what is actually happening here.\n","\n","(**So you don't need to do this when you actually use this object.**)\n","\n","But I do want to show you what it's logging for us in the back end.\n","\n","So we're going to create a basic configuration for logging.\n","\n","And from the logging I'm going to get logger for this lang chain dot."],"metadata":{"id":"pew2L5oPez5S"}},{"cell_type":"code","source":["# Logging Behind Scenes\n","# You don't need to do this when you actually use this object.\n","import logging\n","logging.basicConfig()\n","logging.getLogger('langchain.retrievers.multi_query').setLevel(logging.INFO)\n","\n","# YOU DONOT NEED TO DO THIS\n","# but this is actually going to show us what the generated queries are and\n"],"metadata":{"id":"y-aSKq59aSWP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# THIS WILL NOT DIRECTLY ANSWER ANY QUERY\n","# RETURNS N DOCS THAT ARE MOST SIMILAR/RELEVANT\n","unique_docs = retriever_from_llm.get_relevant_documents(query=question)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8TrU4me3aSZX","executionInfo":{"status":"ok","timestamp":1691724536161,"user_tz":-540,"elapsed":2498,"user":{"displayName":"Gaurav Dahal","userId":"01909755880149023152"}},"outputId":"c1228197-0565-4d7f-96af-c3477342a991"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["INFO:langchain.retrievers.multi_query:Generated queries: ['1. What is the date of declassification for this information?', '2. Can you provide the declassification date for this?', '3. At what time was this information officially declassified?']\n"]}]},{"cell_type":"markdown","source":["Here, we can see that it actually is generating multiple queries to test against the Chroma database."],"metadata":{"id":"ZS4cy2lpg-Kd"}},{"cell_type":"code","source":["len(unique_docs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r0uZ8-SkgmWe","executionInfo":{"status":"ok","timestamp":1691724539867,"user_tz":-540,"elapsed":319,"user":{"displayName":"Gaurav Dahal","userId":"01909755880149023152"}},"outputId":"b7507f32-1cac-464e-8f42-24ea7a376e38"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2"]},"metadata":{},"execution_count":80}]},{"cell_type":"code","source":["print(unique_docs[0].page_content)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_DnDz_GXgmab","executionInfo":{"status":"ok","timestamp":1691724565346,"user_tz":-540,"elapsed":450,"user":{"displayName":"Gaurav Dahal","userId":"01909755880149023152"}},"outputId":"7e841f3b-f5fc-4609-c9a6-cca6e2892107"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["== Background ==\n","In 1974, a New York Times article was published that accused the CIA of illegal operations committed against US citizens. Authored by Seymour M. Hersh, it documented an intelligence operation against the anti-war movement, as well as \"break-ins, wiretapping and the surreptitious inspection of mail\" conducted since the 1950s. According to former CIA Official Cord Meyer, these disclosures \"Convinced large sections of the American public that the CIA had become a domestic Gestapo and stimulated an overwhelming demand for the wide-ranging congressional investigations that were to follow.\"Hersh had been tipped off to the possibility of an \"in house operation\" by an unidentified member of the CIA in spring of 1974. He embarked on an investigation, speaking to sources that included CIA Chief of Counterintelligence James Angleton. Although he was not aware of its existence, Hersh uncovered much information that had been documented in the \"Family Jewels\", a report ordered by Director of Central Intelligence William Colby that chronicled CIA abuses over the past 25 years. The report would not be formally revealed to the public until 2007.\n","\n","\n","=== Monitoring of anti-war movement and Project MINARET ===\n","\n","The article alleged that CIA agents had followed and photographed participants in the antiwar movement, as well as other demonstrations. It also reported that the CIA \"set up a network of informants who were ordered to penetrate antiwar groups\", and even placed an avowedly anti-war congressperson under surveillance while putting other lawmakers in a dossier on dissident Americans.\n","Instituted in 1967 by the NSA, Project MINARET's purpose was to document \"Soviet, Chinese, and North Vietnamese influence over the militant civil rights and anti–Vietnam War movements\" for the CIA and FBI, according to historian Donald Critchlow. The NSA provided CIA and FBI officials  with reports of intercepted international communications by certain individuals in these movements. NSA officials stipulated that FBI and CIA agents must destroy or return these reports within two weeks of receiving them. The NSA also required that \"the reports not be 'identified with the National Security Agency' and that all records relating to this program were 'not serialize[d]' or filed with other NSA records, were classified 'Top Secret,' and were stamped 'Background Use Only'... because t\n"]}]},{"cell_type":"markdown","source":["Here, we are introducing a little bit of randomness by using an LM to create these multi queries.\n","\n","So if you do plan on using a multi query retriever, what I would generally recommend is that you set the **temperature** of your chat model to 0.\n","\n","That way these results are more repeatable for the same query. For repeatability\n","\n","What you wouldn't want is a very high temperature, very random chat model to make queries that are very, very different from the initial ones to have repeatability in process.\n","\n","For eg:    llm = ChatOpenAI(temperature=0)"],"metadata":{"id":"40vlyAUloZrT"}},{"cell_type":"markdown","source":["---------"],"metadata":{"id":"agS2Sahloo1X"}},{"cell_type":"markdown","source":["# **Context Compression**\n","\n","Let's explore how to use large language models to \"compress\" our outputs.\n","\n","---\n","\n","It's going to grab the context that you return as a document from your vector store and then send it to a large language model for \"***compression***\".\n","\n","That is essentially just making the reply smaller or more relevant.\n","\n","---\n","\n","**NOTE:** We're not really performing any sort of compression in the traditional sense, like when you compress something to a zip file.   \n","\n","Instead, really all we're doing is we're using a call to a large language model to grab a larger document text output and distill it to a smaller and more relevant output. (Making it distillation rather than compression)"],"metadata":{"id":"BXZ30-Tb92gD"}},{"cell_type":"code","source":["# importing libraries\n","from langchain.vectorstores import Chroma\n","from langchain.document_loaders import WikipediaLoader\n","from langchain.embeddings.openai import OpenAIEmbeddings\n","from langchain.text_splitter import CharacterTextSplitter"],"metadata":{"id":"ZKbCrcJTgmd5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["db_connection = Chroma(persist_directory='/content/some_new_mkultra',\n","                       embedding_function=embedding_function)"],"metadata":{"id":"ZupIhojdgmhO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# import model we want for the compression (chat_models)\n","# then we need to call retrievers\n","# then we need LLM Chain extractor which uses chains on the backend\n","\n","from langchain.chat_models import ChatOpenAI\n","from langchain.retrievers import ContextualCompressionRetriever\n","from langchain.retrievers.document_compressors import LLMChainExtractor"],"metadata":{"id":"STPeccefgmky"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ORDER TO USE COMPRESSION RETRIEVER\n","\n","# Figure out which LLM Use for Cmpression\n","llm = ChatOpenAI(temperature=0)\n","\n","# Insert LLM -> LLMChainExtractor\n","compressor = LLMChainExtractor.from_llm(llm)\n","\n","### Contextual Compression\n","compression_retriever = ContextualCompressionRetriever(base_compressor = compressor,\n","                                                       base_retriever=db_connection.as_retriever())"],"metadata":{"id":"Sg54JiSjgmnw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Here, we passed in the compressor and the connection as a retriever, and now we have a compression retriever.\n","\n","It's gonna retrieve something and then also have the ability to compress it."],"metadata":{"id":"h1MNo_fbFKNo"}},{"cell_type":"code","source":["docs = db_connection.similarity_search(\"When was this declassified?\")\n","docs[0].page_content"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":244},"id":"k-05zeh2G5Jh","executionInfo":{"status":"ok","timestamp":1691732433540,"user_tz":-540,"elapsed":784,"user":{"displayName":"Gaurav Dahal","userId":"01909755880149023152"}},"outputId":"4e82eb56-b186-4e03-f1d5-687aa5ac63de"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'== Background ==\\nIn 1974, a New York Times article was published that accused the CIA of illegal operations committed against US citizens. Authored by Seymour M. Hersh, it documented an intelligence operation against the anti-war movement, as well as \"break-ins, wiretapping and the surreptitious inspection of mail\" conducted since the 1950s. According to former CIA Official Cord Meyer, these disclosures \"Convinced large sections of the American public that the CIA had become a domestic Gestapo and stimulated an overwhelming demand for the wide-ranging congressional investigations that were to follow.\"Hersh had been tipped off to the possibility of an \"in house operation\" by an unidentified member of the CIA in spring of 1974. He embarked on an investigation, speaking to sources that included CIA Chief of Counterintelligence James Angleton. Although he was not aware of its existence, Hersh uncovered much information that had been documented in the \"Family Jewels\", a report ordered by Director of Central Intelligence William Colby that chronicled CIA abuses over the past 25 years. The report would not be formally revealed to the public until 2007.\\n\\n\\n=== Monitoring of anti-war movement and Project MINARET ===\\n\\nThe article alleged that CIA agents had followed and photographed participants in the antiwar movement, as well as other demonstrations. It also reported that the CIA \"set up a network of informants who were ordered to penetrate antiwar groups\", and even placed an avowedly anti-war congressperson under surveillance while putting other lawmakers in a dossier on dissident Americans.\\nInstituted in 1967 by the NSA, Project MINARET\\'s purpose was to document \"Soviet, Chinese, and North Vietnamese influence over the militant civil rights and anti–Vietnam War movements\" for the CIA and FBI, according to historian Donald Critchlow. The NSA provided CIA and FBI officials  with reports of intercepted international communications by certain individuals in these movements. NSA officials stipulated that FBI and CIA agents must destroy or return these reports within two weeks of receiving them. The NSA also required that \"the reports not be \\'identified with the National Security Agency\\' and that all records relating to this program were \\'not serialize[d]\\' or filed with other NSA records, were classified \\'Top Secret,\\' and were stamped \\'Background Use Only\\'... because t'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":45}]},{"cell_type":"code","source":["compressed_docs = compression_retriever.get_relevant_documents(\"When was this declaassified?\")"],"metadata":{"id":"Yb249_Xegmq6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1691732449418,"user_tz":-540,"elapsed":4752,"user":{"displayName":"Gaurav Dahal","userId":"01909755880149023152"}},"outputId":"3c094f29-c07a-44fb-a4b8-a66b66adae59"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:275: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["compressed_docs[0].page_content"],"metadata":{"id":"xzCULptIgm8l","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1691732451846,"user_tz":-540,"elapsed":317,"user":{"displayName":"Gaurav Dahal","userId":"01909755880149023152"}},"outputId":"a0789b01-e18f-4f04-813c-26e09bee3669"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'The commission issued a single report in 1975.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":47}]},{"cell_type":"code","source":["compressed_docs[0].metadata[\"summary\"]"],"metadata":{"id":"DC97RHGXgm_q","colab":{"base_uri":"https://localhost:8080/","height":365},"executionInfo":{"status":"ok","timestamp":1691732502496,"user_tz":-540,"elapsed":333,"user":{"displayName":"Gaurav Dahal","userId":"01909755880149023152"}},"outputId":"c31f41c2-2571-4b31-cc22-fc79c8b8a2ba"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'The United States President\\'s Commission on CIA Activities within the United States was ordained by President Gerald Ford in 1975 to investigate the activities of the Central Intelligence Agency and other intelligence agencies within the United States. The Presidential Commission was led by Vice President Nelson Rockefeller, from whom it gained the nickname the Rockefeller Commission.\\nThe commission was created in response to a December 1974 report in The New York Times that the CIA had conducted illegal domestic activities, including experiments on US citizens, during the 1960s. The commission issued a single report in 1975, touching upon certain CIA abuses including mail opening and surveillance of domestic dissident groups. It also publicized Project MKUltra, a CIA mind control research program.\\nSeveral weeks later, committees were established in the House and Senate for a similar purpose. White House Personnel, including future Vice President Dick Cheney, edited the results, excluding many of the commission\\'s findings from the final report. Some of these findings were included in later reports by the Congressional Committees.\\nBefore it was even released, the report faced scrutiny from the media, and was deemed a \"whitewash.\" The investigation was intended to be independent of Presidential interference, but the findings and recommendations included in the final report were highly altered from what was chosen by the commission itself. It was ultimately superseded in notability by the more substantial Church Committee in what became known as the \"Year of Intelligence.\"'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":48}]},{"cell_type":"code","source":[],"metadata":{"id":"vj8P5vkPgnC8"},"execution_count":null,"outputs":[]}]}